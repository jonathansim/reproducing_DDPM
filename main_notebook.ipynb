{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for the setup ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math \n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import numpy as np\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, datasets\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import wandb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNET ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Positional Encoding ###\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    '''\n",
    "    From spmallick on GitHub\n",
    "\n",
    "    - chose to use this over own implementation as it is more efficient (due to precomputing the embeddings, allowing for faster training)\n",
    "    '''\n",
    "    def __init__(self, total_time_steps=1000, time_emb_dims=128, time_emb_dims_exp=512):\n",
    "        super().__init__()\n",
    "\n",
    "        half_dim = time_emb_dims // 2\n",
    "\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "\n",
    "        ts = torch.arange(total_time_steps, dtype=torch.float32)\n",
    "\n",
    "        emb = torch.unsqueeze(ts, dim=-1) * torch.unsqueeze(emb, dim=0)\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "\n",
    "        self.time_blocks = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(emb),\n",
    "            nn.Linear(in_features=time_emb_dims, out_features=time_emb_dims_exp),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(in_features=time_emb_dims_exp, out_features=time_emb_dims_exp),\n",
    "        )\n",
    "\n",
    "    def forward(self, time):\n",
    "        return self.time_blocks(time)\n",
    "\n",
    "\n",
    "### SUBMODULES ### \n",
    "# Block for self attention mechanism\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dropout=0.0, dim_scale = 0.5):\n",
    "        '''\n",
    "        Attention Block with flexible dimension adjustment.\n",
    "        \n",
    "        Args:\n",
    "            dim: Number of input channels.\n",
    "            num_heads: Number of attention heads.\n",
    "            dim_scale: Scale factor for input dimensions.\n",
    "                        - Use dim_scale=0.5 for downblock.\n",
    "                        - Use dim_scale=1 for upblock.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.scaled_dim = int(dim * dim_scale)\n",
    "        self.norm = nn.LayerNorm(self.scaled_dim)\n",
    "        self.mhsa = nn.MultiheadAttention(self.scaled_dim, num_heads=heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Reshape for multihead attention\n",
    "        x_flat = x.view(B, C, -1).transpose(1, 2) # shape (B, H*W, C)\n",
    "\n",
    "        # normalize and apply multihead attention\n",
    "        x_norm = self.norm(x_flat)\n",
    "        attention_out = self.mhsa(x_norm, x_norm, x_norm)[0]\n",
    "\n",
    "        # Add residual connection\n",
    "        x = x_flat + attention_out # we do this to ensure that no information from the input is lost (no matter how the attention might have changed the input)\n",
    "\n",
    "        # Reshape back to original shape\n",
    "        x = x.transpose(1, 2).view(B, C, H, W)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Block for downsampling part of U-net\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_emb_dims, dropout=0.1, use_attention=False, heads=4):\n",
    "        super().__init__()\n",
    "        # First conv\n",
    "        self.norm1 = nn.GroupNorm(8, in_channels)\n",
    "        self.activation1 = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels//2, kernel_size=3, padding=1)\n",
    " \n",
    "        # Second conv \n",
    "        self.norm2 = nn.GroupNorm(8, out_channels//2)\n",
    "        self.activation2 = nn.SiLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels//2, out_channels//2, kernel_size=3, padding=1)\n",
    "\n",
    "        # Dropout, downsample, timestep embedding, and self-attention\n",
    "        self.dropout = nn.Dropout2d(p=dropout)\n",
    "        self.downsample = nn.Conv2d(out_channels//2, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.time_proj = nn.Linear(time_emb_dims, out_channels//2)\n",
    "        if use_attention:\n",
    "            self.attention = SelfAttentionBlock(dim=out_channels, heads=heads, dim_scale=0.5)\n",
    "        else:\n",
    "            self.attention = nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb):\n",
    "        # Feed time embedding through linear layer\n",
    "        time_emb_proj = self.time_proj(time_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "  \n",
    "        # Normalize and apply activation\n",
    "        x = self.activation1(self.norm1(x))\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # Add timestep embedding\n",
    "        x += self.activation1(time_emb_proj)\n",
    "\n",
    "        # Normalize and apply activation\n",
    "        x = self.activation2(self.norm2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # Apply self-attention\n",
    "        x = self.attention(x)\n",
    "\n",
    "        skip = x  # Save for skip connection   \n",
    "        x = self.downsample(x)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "# Block for bottleneck part of U-net\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, channels, time_emb_dims, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # First conv\n",
    "        self.norm1 = nn.GroupNorm(8, channels)\n",
    "        self.activation1 = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Second conv\n",
    "        self.norm2 = nn.GroupNorm(8, channels)\n",
    "        self.activation2 = nn.SiLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dropout = nn.Dropout2d(p=dropout)\n",
    "        self.time_proj = nn.Linear(time_emb_dims, channels)\n",
    "\n",
    "    def forward(self, x, time_emb):\n",
    "        # Feed time embedding through linear layer\n",
    "        time_emb_proj = self.time_proj(time_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "  \n",
    "        # Normalize and apply activation\n",
    "        x = self.activation1(self.norm1(x))\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # Add timestep embedding\n",
    "        x += self.activation1(time_emb_proj)\n",
    "\n",
    "        # Normalize and apply activation\n",
    "        x = self.activation2(self.norm2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Block for upsampling part of U-net\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_emb_dims, dropout=0.1, use_attention=False, heads=4):\n",
    "        super().__init__()\n",
    "        # First conv\n",
    "        self.norm1 = nn.GroupNorm(8, in_channels)\n",
    "        self.activation1 = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # Second conv\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.activation2 = nn.SiLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upsample = nn.ConvTranspose2d(out_channels*2, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.time_proj = nn.Linear(time_emb_dims, out_channels)\n",
    "        self.dropout = nn.Dropout2d(p=dropout)\n",
    "        \n",
    "        if use_attention:\n",
    "            self.attention = SelfAttentionBlock(dim=out_channels, heads=heads, dim_scale=1)\n",
    "        else:\n",
    "            self.attention = nn.Identity()\n",
    "\n",
    "    def forward(self, x, skip, time_emb):\n",
    "        # Upsample\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        # Feed time embedding through linear layer\n",
    "        time_emb_proj = self.time_proj(time_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # Resize if spatial dimensions mismatch\n",
    "        if x.shape[-2:] != skip.shape[-2:]:\n",
    "            x = F.interpolate(x, size=skip.shape[-2:], mode='nearest')\n",
    "        \n",
    "        # Add skip connection\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "\n",
    "        # Normalize and apply activation\n",
    "        x = self.activation1(self.norm1(x))\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # Add timestep embedding\n",
    "        x += self.activation1(time_emb_proj)\n",
    "\n",
    "        # Normalize and apply activation\n",
    "        x = self.activation2(self.norm2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # Apply self-attention\n",
    "        x = self.attention(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "### U-NET MODEL ###\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels=1, resolutions=[64, 128, 256, 512], time_emb_dims=512, dropout=0.1, use_attention=[False, True, False], heads=4):\n",
    "        \"\"\"\n",
    "        U-Net implementation for DDPM\n",
    "        Args:\n",
    "            input_channels: Number of input channels (e.g., 1 for MNIST).\n",
    "            base_channels: Number of channels in the first convolution layer.\n",
    "            num_resolutions: Number of downsampling/upsampling blocks.\n",
    "            time_emb_dims: Dimensionality of timestep embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_channels = resolutions[0]\n",
    "\n",
    "\n",
    "        # Input layer\n",
    "        self.input_conv = nn.Conv2d(input_channels, self.base_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # Downsampling blocks\n",
    "        self.down_blocks = nn.ModuleList([\n",
    "            DownBlock(\n",
    "                in_channels=resolutions[i],\n",
    "                out_channels=resolutions[i+1],\n",
    "                time_emb_dims=time_emb_dims,\n",
    "                dropout=dropout, \n",
    "                use_attention=use_attention[i],\n",
    "                heads=heads\n",
    "            )\n",
    "            for i in range(len(resolutions) - 1)\n",
    "        ])\n",
    "\n",
    "\n",
    "        # Bottleneck block\n",
    "        self.bottleneck = BottleneckBlock(channels=resolutions[-1], time_emb_dims=time_emb_dims, dropout=dropout)\n",
    "\n",
    "        # Upsampling blocks\n",
    "        self.up_blocks = nn.ModuleList([\n",
    "            UpBlock(\n",
    "                in_channels=resolutions[i + 1],\n",
    "                out_channels=resolutions[i],\n",
    "                time_emb_dims=time_emb_dims,\n",
    "                dropout=dropout, \n",
    "                use_attention=use_attention[i],\n",
    "                heads=heads\n",
    "            )\n",
    "            for i in reversed(range(len(resolutions) - 1))\n",
    "        ])\n",
    "\n",
    "        # Output layer \n",
    "        self.output_conv = nn.Conv2d(self.base_channels, input_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, time_emb):\n",
    "        \"\"\"\n",
    "        Forward pass of the U-Net.\n",
    "        Args:\n",
    "            x: Input image tensor of shape (batch_size, input_channels, height, width).\n",
    "            time_emb: Timestep embedding tensor of shape (batch_size, time_emb_dims).\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, input_channels, height, width).\n",
    "        \"\"\"\n",
    "\n",
    "        # Input conv \n",
    "        x = self.input_conv(x)\n",
    "\n",
    "        # Downsampling path \n",
    "        skips = []\n",
    "        for block in self.down_blocks:\n",
    "            x, skip = block(x, time_emb)\n",
    "            skips.append(skip)\n",
    "        \n",
    "        #print(f\"Skips: {len(skips)}\")\n",
    "        # Bottleneck block\n",
    "        x = self.bottleneck(x, time_emb)\n",
    "\n",
    "        # Upsampling path\n",
    "        for block, skip in zip(self.up_blocks, reversed(skips)):\n",
    "            x = block(x, skip, time_emb) \n",
    "        \n",
    "        # Output conv\n",
    "        x = self.output_conv(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion Model\n",
    "The DDPM setup consists of the UNET defined above, a Diffusion class implementing forward and reverse processes, a training function implementing alg. 1 and a sampling func. implementing alg. 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion: \n",
    "    def __init__(self, T=1000, beta_min=10e-5, beta_max=0.02, schedule='linear', device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the diffusion process.\n",
    "        Args:\n",
    "            T: Total number of timesteps.\n",
    "            beta_min: Minimum value of beta in the noise schedule.\n",
    "            beta_max: Maximum value of beta in the noise schedule.\n",
    "            schedule: Type of noise schedule ('linear', 'cosine', etc.).\n",
    "            device: Device to use for computations.\n",
    "        \"\"\"\n",
    "        self.T = T\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "        self.schedule = schedule\n",
    "        self.device = device\n",
    "\n",
    "        # Noise schedule parameters\n",
    "        self.beta = None\n",
    "        self.alpha = None\n",
    "        self.alpha_bar = None\n",
    "\n",
    "        # Precompute schedule\n",
    "        self.get_noise_schedule()\n",
    "\n",
    "    def get_noise_schedule(self):\n",
    "        \"\"\"\n",
    "        Precompute the noise schedule.\n",
    "        \"\"\"\n",
    "        if self.schedule == 'linear':\n",
    "            self.beta = torch.linspace(self.beta_min, self.beta_max, self.T).to(self.device)\n",
    "            self.alpha = (1 - self.beta).to(self.device)\n",
    "            self.alpha_bar = self.alpha.cumprod(dim=0).to(self.device)\n",
    "\n",
    "        elif self.schedule == 'cosine':\n",
    "            steps = torch.linspace(0, torch.pi, self.T).to(self.device)\n",
    "            self.beta = ((torch.cos(steps) + 1) * 0.5 * (self.beta_max - self.beta_min) + self.beta_min).flip(0)\n",
    "            self.alpha = (1 - self.beta).to(self.device)\n",
    "            self.alpha_bar = self.alpha.cumprod(dim=0).to(self.device)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown schedule type: {self.schedule}\")\n",
    "        \n",
    "    def forward_diffusion(self, x0, t):\n",
    "        \"\"\"\n",
    "        Perform the forward diffusion process.\n",
    "        Args:\n",
    "            x0: Original data (e.g., images).\n",
    "            t: Timesteps (tensor of integers).\n",
    "        Returns:\n",
    "            xt: Noisy data at timestep t.\n",
    "            epsilon: The noise added.\n",
    "        \"\"\"\n",
    "        eps = torch.randn_like(x0)\n",
    "\n",
    "        sqrt_alpha_bar_t = self.alpha_bar[t].sqrt().view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar_t = (1 - self.alpha_bar[t]).sqrt().view(-1, 1, 1, 1)\n",
    "\n",
    "        xt = sqrt_alpha_bar_t * x0 + sqrt_one_minus_alpha_bar_t * eps\n",
    " \n",
    "        return xt, eps \n",
    "    \n",
    "    def reverse_diffusion(self, xt, t, model, time_embedding):\n",
    "        \"\"\"\n",
    "        Perform the reverse diffusion process.\n",
    "        Args:\n",
    "            xt: Noisy data at timestep t.\n",
    "            t: Timesteps for batch (tensor of integers).\n",
    "            model: Model used for reverse diffusion.\n",
    "            time_embedding: Time embedding object.\n",
    "        Returns:\n",
    "            xr: Reconstructed data at timestep t.\n",
    "        \"\"\"\n",
    "        z = torch.where((t > 1).view(-1, 1, 1, 1), torch.randn_like(xt), torch.zeros_like(xt)) \n",
    "        \n",
    "        # extract time embedding\n",
    "        time_emb = time_embedding(t)\n",
    "\n",
    "        # predict noise\n",
    "        eps_theta = model(xt, time_emb)\n",
    "\n",
    "        sqrt_alpha_t = self.alpha[t].sqrt().view(-1, 1, 1, 1)  # we need to reshape the tensor to match the shape of xt (same goes for the other tensors)\n",
    "        beta_t = self.beta[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar_t = (1 - self.alpha_bar[t]).sqrt().view(-1, 1, 1, 1) \n",
    "        sigma_t = beta_t.sqrt() \n",
    "\n",
    "        xt_minus_one = 1. / sqrt_alpha_t * (xt - (beta_t / sqrt_one_minus_alpha_bar_t) * eps_theta) + sigma_t * z\n",
    "\n",
    "        return xt_minus_one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for training DDPM ###\n",
    "def train_ddpm_epoch(model: object, diffusion: object, time_embedding: object, train_loader: object, epoch: int, device: str, optimizer: object, lr_scheduler: object):\n",
    "    \"\"\"\n",
    "    This function implements Algorithm 1 from the paper, specifically it trains the U-Net model for one epoch.\n",
    "    Args:\n",
    "        model: U-Net model.\n",
    "        diffusion: Diffusion object.\n",
    "        time_embedding: Time embedding object.\n",
    "        train_loader: DataLoader for training data.\n",
    "        epoch: Current epoch.\n",
    "        device: Device to use (e.g., 'cuda' or 'cpu').\n",
    "        optimizer: Optimizer for training.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Training epoch {epoch}...\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, (x0, _) in enumerate(train_loader):\n",
    "        x0 = x0.to(device)\n",
    "\n",
    "        # Sample random timestep\n",
    "        t = torch.randint(0, diffusion.T, (x0.size(0),), device=device)\n",
    "\n",
    "        # Get sinusoidal time embedding\n",
    "        time_emb = time_embedding(t)\n",
    "\n",
    "        # Forward diffusion\n",
    "        xt, epsilon = diffusion.forward_diffusion(x0, t)\n",
    "\n",
    "        # Predict noise\n",
    "        epsilon_pred = model(xt, time_emb)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(epsilon_pred, epsilon)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Use learning rate scheduler\n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        # Log loss\n",
    "        wandb.log({\"loss\": loss.item(), \"epoch\": epoch, \"lr\": optimizer.param_groups[0]['lr']})\n",
    "\n",
    "        if epoch > 1: \n",
    "            wandb.log({\"loss after 1st epoch\": loss.item()})\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, batch {i}: loss={loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to sample from DDPM ###\n",
    "\n",
    "def sample_ddpm(model: object, diffusion: object, time_embedding: object, device: str, num_samples: int = 16, dataset: str = 'MNIST'):\n",
    "    \"\"\"\n",
    "    This function implements Algorithm 2 from the paper, specifically it samples from the U-Net model.\n",
    "    Args:\n",
    "        model: U-Net model.\n",
    "        diffusion: Diffusion object.\n",
    "        time_embedding: Time embedding object.\n",
    "        device: Device to use (e.g., 'cuda' or 'cpu').\n",
    "        num_samples: Number of samples to generate.\n",
    "        dataset: Dataset to use ('MNIST' or 'CIFAR10').\n",
    "    Returns:\n",
    "        samples: Generated samples.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Sampling {num_samples} samples...\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1. Initialize samples from standard gaussian distribution\n",
    "        if dataset == 'MNIST':\n",
    "            x = torch.randn((num_samples, 1, 28, 28), device=device)\n",
    "        elif dataset == 'CIFAR10':\n",
    "            x = torch.randn((num_samples, 3, 32, 32), device=device)\n",
    "\n",
    "        # 2. iterative reverse diffusion \n",
    "        for t in reversed(range(1, diffusion.T + 1)):\n",
    "            # prepare timestep tensor\n",
    "            t_tensor = torch.full((num_samples,), t - 1, device=device, dtype=torch.long)\n",
    "\n",
    "            # Perform reverse diffusion step\n",
    "            x = diffusion.reverse_diffusion(x, t_tensor, model, time_embedding)\n",
    "        \n",
    "        # 3. Return samples\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataloader ###\n",
    "def get_dataloader(dataset = \"MNIST\", batch_size = 128):\n",
    "    transforms = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                # No flips in MNIST as it disturbes the interpretation of numbers?\n",
    "                #torchvision.transforms.Resize((32, 32), interpolation=torchvision.transforms.InterpolationMode.BICUBIC), # TODO: Resize to 32 x 32 for unet implementation?\n",
    "                torchvision.transforms.Normalize((0.5,), (0.5,))  # Correpsonds to scaling between [-1, 1] --> (x - 0.5)/0.5\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    transforms_cifar10_train = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.RandomHorizontalFlip(),\n",
    "                #torchvision.transforms.Resize((32, 32), interpolation=torchvision.transforms.InterpolationMode.BICUBIC), # TODO: Match paper or resize to 28 x 28 for unet implementation\n",
    "                torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Scaling between [-1, 1]\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    transforms_cifar10_test = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                #torchvision.transforms.Resize((32, 32), interpolation=torchvision.transforms.InterpolationMode.BICUBIC), # TODO: Match paper or resize to 28 x 28 for unet implementation\n",
    "                torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Scaling between [-1, 1]\n",
    "            ]\n",
    "        )\n",
    "    if dataset == \"MNIST\":\n",
    "        trainset = MNIST(\"./temp/\", train=True, download=True, transform= transforms)\n",
    "        testset = MNIST(\"./temp/\", train=False, download=True, transform= transforms)\n",
    "    elif dataset == \"CIFAR10\":\n",
    "        trainset = CIFAR10(\"./temp/\", train=True, download=True, transform= transforms_cifar10_train)\n",
    "        testset = CIFAR10(\"./temp/\", train=False, download=True, transform= transforms_cifar10_test)\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(trainset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "    test_dataloader = DataLoader(testset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "def get_dataloader_evaluation(dataset = \"MNIST\", batch_size = 128):\n",
    "    transforms_mnist = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "            torchvision.transforms.Resize((299, 299)),  # Resize for InceptionV3\n",
    "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  #normalize for InceptionV3\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    transforms_cifar10 = torchvision.transforms.Compose(\n",
    "          [\n",
    "              torchvision.transforms.Resize((299, 299)),  # Resize for InceptionV3\n",
    "              torchvision.transforms.ToTensor(),\n",
    "              torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # CIFAR normalization\n",
    "          ])\n",
    "    \n",
    "\n",
    "    if dataset == \"MNIST\":\n",
    "        trainset = MNIST(\"./temp/\", train=True, download=True, transform= transforms_mnist)\n",
    "    elif dataset == \"CIFAR10\":\n",
    "        trainset = CIFAR10(\"./temp/\", train=True, download=True, transform= transforms_cifar10)\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(trainset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Learning Rate Scheduler ###\n",
    "class WarmUpPiecewiseConstantSchedule(_LRScheduler):\n",
    "    '''\n",
    "    This class implements a learning rate scheduler that combines a warm-up phase with a piecewise constant decay.\n",
    "    That is, the learning rate is increased linearly from 0 to the base learning rate during the warm-up phase, and then \n",
    "    decreased by a factor of decay_ratio at each epoch in decay_epochs (similarly to MultiStepLR).\n",
    "    Note: can maybe be done better, but needed it done quickly... \n",
    "\n",
    "    Note: I implemented this class as part of my bachelor's thesis (jonathansim). \n",
    "    '''\n",
    "    def __init__(self, optimizer, steps_per_epoch, base_lr, lr_decay_ratio, lr_decay_epochs, warmup_epochs, last_epoch=-1):\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.base_lr = base_lr\n",
    "        self.decay_ratio = lr_decay_ratio\n",
    "        self.decay_epochs = lr_decay_epochs\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.decay_steps = [e * steps_per_epoch for e in lr_decay_epochs]  # Convert epochs to steps\n",
    "        super(WarmUpPiecewiseConstantSchedule, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        # Calculate the current step\n",
    "        lr_step = self.last_epoch\n",
    "        lr_epoch = lr_step / self.steps_per_epoch\n",
    "        learning_rate = self.base_lr\n",
    "\n",
    "        # Warm-Up Phase\n",
    "        if lr_epoch < self.warmup_epochs:\n",
    "            learning_rate = self.base_lr * lr_step / (self.warmup_epochs * self.steps_per_epoch)\n",
    "        else:\n",
    "            # Piecewise Constant Decay Phase\n",
    "            for i, start_step in enumerate(self.decay_steps):\n",
    "                if lr_step >= start_step:\n",
    "                    learning_rate = self.base_lr * (self.decay_ratio ** (i + 1))\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        return [learning_rate for _ in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for setting seed ###\n",
    "def set_training_seed(seed):\n",
    "    # Function to set the different seeds \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Frechet Inception Distance ### \n",
    "def calculate_fid(feat1, feat2):\n",
    "    '''\n",
    "    calculate fid between two sets of images\n",
    "    feat1: real images - feature vector obtained from inception model (or alternative model)\n",
    "    feat2: generated images - feature vector obtained from inception model (or alternative model)\n",
    "    return: fid score\n",
    "    '''\n",
    "    # calculate mean and covariance of features\n",
    "    mu1, sigma1 = np.mean(feat1, axis=0), np.cov(feat1, rowvar=False) \n",
    "    mu2, sigma2 = np.mean(feat2, axis=0), np.cov(feat2, rowvar=False)\n",
    "    print(\"Mean1: \", np.shape(mu1))\n",
    "    print(\"Mean2: \", np.shape(mu2))\n",
    "    # sum squared difference between means\n",
    "    diff = np.sum((mu1 - mu2)**2)\n",
    "    print(\"diff: \", diff)\n",
    "    # calculate \"geometric mean\" of covariance matrices\n",
    "    covmean = fractional_matrix_power(sigma1.dot(sigma2), 0.5)\n",
    "    # check for imaginary numbers\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = np.real(covmean)\n",
    "\n",
    "    fid = diff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "\n",
    "def compute_activations(samples, model, batched = False, batch_size = 100):\n",
    "    \"\"\"\n",
    "    Compute activations from samples based on model.\n",
    "    Args:\n",
    "        samples: Tensor of shape (num_samples, channels, height, width)\n",
    "        model: Pre-trained model to extract features\n",
    "    Returns:\n",
    "        activations: Feature activations for input images\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    samples = samples.to(next(model.parameters()).device)  # Move to the same device as the model\n",
    "\n",
    "    if batched:\n",
    "        activations = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, samples.size(0), batch_size):\n",
    "                batch = samples[i:i+batch_size]\n",
    "                batch_activations = model(batch).cpu().numpy()\n",
    "                activations.append(batch_activations)\n",
    "        return np.concatenate(activations, axis=0)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            activations = model(samples)\n",
    "        return activations.cpu().numpy()\n",
    "\n",
    "\n",
    "def get_images_in_batches(dataloader, num_images=None):\n",
    "    \"\"\"\n",
    "    Fetch e.g. CIFAR-10 images in smaller batches without concatenating them.\n",
    "    Args:\n",
    "        dataloader: PyTorch DataLoader providing the dataset.\n",
    "        num_images: Number of images to fetch.\n",
    "    Yields:\n",
    "        Batches of real images from the dataset.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for images, _ in dataloader:\n",
    "        count += len(images)\n",
    "        if num_images and count >= num_images:\n",
    "            yield images\n",
    "            break\n",
    "\n",
    "        # Yield images batch-by-batch\n",
    "        yield images\n",
    "\n",
    "        # Free memory\n",
    "        del images\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def full_fid(generated_samples, data, num_images = 10000):\n",
    "    \"\"\"\n",
    "    Computes the Fr√©chet Inception Distance (FID) between generated images and real images.\n",
    "    Args:\n",
    "        generated_samples (torch.Tensor): Tensor of generated samples \n",
    "            with shape (num_samples, channels, height, width).\n",
    "        data (str): MNIST or CIFAR10\n",
    "        num_images (int, optional): Number of real images to use for FID computation. \n",
    "            Defaults to 10,000.\n",
    "    Returns:\n",
    "        float: FID score between the generated and real images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load pre-trained InceptionV3\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    inception_classifier = models.inception_v3(weights='DEFAULT', transform_input=False)\n",
    "    inception_classifier.fc = torch.nn.Identity()  # remove classification layer to extract features\n",
    "    inception_classifier = inception_classifier.to(device)\n",
    "\n",
    "    # Get train samples\n",
    "    dataloader = get_dataloader_evaluation(dataset = data, batch_size = 100)\n",
    "    print(\"DATA: \", data)\n",
    "    # Get activations for real images\n",
    "    real_image_activations = []\n",
    "    for batch in get_images_in_batches(dataloader, num_images=num_images):\n",
    "        activation = compute_activations(batch, inception_classifier)\n",
    "        real_image_activations.extend(activation)\n",
    "    real_image_activations = real_image_activations[:num_images]\n",
    "\n",
    "    # Get activations of generated images\n",
    "    min_val = generated_samples.min()\n",
    "    max_val = generated_samples.max()\n",
    "    resized_samples = (generated_samples - min_val) / (max_val - min_val) # rescale pixel values to [0,1]\n",
    "    if data == \"MNIST\":\n",
    "        resized_samples = resized_samples.repeat(1, 3, 1, 1)\n",
    "    resized_samples = torchvision.transforms.functional.resize(resized_samples, size = (299, 299)) # resize for inception\n",
    "    resized_samples = torchvision.transforms.functional.normalize(resized_samples, \n",
    "        mean=torch.tensor([0.485, 0.456, 0.406]).view(1, -1, 1, 1).to(device), \n",
    "        std=torch.tensor([0.229, 0.224, 0.225]).view(1, -1, 1, 1).to(device)\n",
    "    )\n",
    "    generated_image_activations = compute_activations(resized_samples, inception_classifier, batched = True)\n",
    "    print(np.shape(generated_image_activations))\n",
    "    print(np.shape(real_image_activations))\n",
    "    fid = calculate_fid(real_image_activations, generated_image_activations)\n",
    "\n",
    "    return fid\n",
    "\n",
    "# full_fid splitted up\n",
    "def get_inception_model():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    inception_classifier = models.inception_v3(weights='DEFAULT', transform_input=False)\n",
    "    inception_classifier.fc = torch.nn.Identity()  # remove classification layer to extract features\n",
    "    inception_classifier = inception_classifier.to(device)\n",
    "    return inception_classifier\n",
    "\n",
    "\n",
    "def get_real_image_activations(inception_classifier, data, num_images = 10000):\n",
    "    # Get train samples\n",
    "    dataloader = get_dataloader_evaluation(dataset = data, batch_size = 100)\n",
    "    print(\"DATA: \", data)\n",
    "    # Get activations for real images\n",
    "    real_image_activations = []\n",
    "    for batch in get_images_in_batches(dataloader, num_images=num_images):\n",
    "        activation = compute_activations(batch, inception_classifier)\n",
    "        real_image_activations.extend(activation)\n",
    "    real_image_activations = real_image_activations[:num_images]\n",
    "    return real_image_activations\n",
    "\n",
    "\n",
    "def calculate_fid_generated_samples(generated_samples, real_image_activations, inception_classifier, device, data, num_images = 10000):\n",
    "    # Get activations of generated images\n",
    "    min_val = generated_samples.min()\n",
    "    max_val = generated_samples.max()\n",
    "    resized_samples = (generated_samples - min_val) / (max_val - min_val) # rescale pixel values to [0,1]\n",
    "    if data == \"MNIST\":\n",
    "        resized_samples = resized_samples.repeat(1, 3, 1, 1)\n",
    "    resized_samples = torchvision.transforms.functional.resize(resized_samples, size = (299, 299)) # resize for inception\n",
    "    resized_samples = torchvision.transforms.functional.normalize(resized_samples, \n",
    "        mean=torch.tensor([0.485, 0.456, 0.406]).view(1, -1, 1, 1).to(device), \n",
    "        std=torch.tensor([0.229, 0.224, 0.225]).view(1, -1, 1, 1).to(device)\n",
    "    )\n",
    "    generated_image_activations = compute_activations(resized_samples, inception_classifier, batched = True)\n",
    "    print(np.shape(generated_image_activations))\n",
    "    print(np.shape(real_image_activations))\n",
    "    fid = calculate_fid(real_image_activations, generated_image_activations)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inception Score ###\n",
    "def calculate_inception_score(samples, dataset, device, batch_size=32, splits=10):\n",
    "    \"\"\"\n",
    "    Calculates the Inception Score for generated samples.\n",
    "    Inspired by: https://github.com/sbarratt/inception-score-pytorch/blob/master/inception_score.py\n",
    "    and paper: https://papers.nips.cc/paper_files/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf\n",
    "\n",
    "    Args:\n",
    "        samples (torch.Tensor): Generated samples with shape (num_samples, channels, height, width).\n",
    "        device (torch.device): Device to perform calculations on\n",
    "        dataset (str): One of the two datasets\n",
    "        batch_size (int): Batch size for processing samples.\n",
    "        splits (int): Number of splits for calculating the mean and standard deviation of the score.\n",
    "\n",
    "    Returns:\n",
    "        float, float: Mean and standard deviation of the Inception Score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load pretrained InceptionV3 model\n",
    "    inception_model = models.inception_v3(pretrained=True)  # keep the final classification layer\n",
    "    inception_model = inception_model.to(device)\n",
    "\n",
    "    inception_model.eval()\n",
    "    samples = samples.to(device)\n",
    "    num_samples = samples.size(0)\n",
    "\n",
    "    # Placeholder for storing predictions\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch = samples[i:i + batch_size]\n",
    "            if dataset == \"MNIST\":\n",
    "                batch = batch.repeat(1, 3, 1, 1)  # repeat channels for Inception when using MNIST\n",
    "            batch = transforms.functional.resize(batch, size=(299, 299))  # resize for Inception\n",
    "            batch = transforms.functional.normalize(batch,\n",
    "                mean=torch.tensor([0.485, 0.456, 0.406]).to(device),\n",
    "                std=torch.tensor([0.229, 0.224, 0.225]).to(device),\n",
    "            )\n",
    "            pred = inception_model(batch)\n",
    "            pred = F.softmax(pred, dim=1)\n",
    "            preds.append(pred.cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "    # Compute IS\n",
    "    scores = []\n",
    "    split_size = preds.shape[0] // splits\n",
    "    for i in range(splits):\n",
    "        part = preds[i * split_size: (i + 1) * split_size] # probability of labels conditioned on image\n",
    "        p_y = np.mean(part, axis=0)\n",
    "        kl_div = part * (np.log(part) - np.log(p_y[None, :]))\n",
    "        kl_div = np.sum(kl_div, axis=1)\n",
    "        scores.append(np.exp(np.mean(kl_div)))\n",
    "\n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of Models ##\n",
    "Since we run various experiments with two different datasets MNIST and CIFAR10 and different parameters, we made a general pipeline for training and evaluating our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main training script ###\n",
    "\n",
    "## Argument parser\n",
    "parser = argparse.ArgumentParser(description='Train (and sample from) DDPM framework.')\n",
    "\n",
    "## Add arguments\n",
    "parser.add_argument('--T', type=int, default=1000, help='Total timesteps.')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='Batch size.')\n",
    "parser.add_argument('--num_epochs', type=int, default=5, help='Number of epochs.')\n",
    "parser.add_argument('--lr', type=float, default=2e-4, help='Learning rate.')\n",
    "parser.add_argument('--dataset', type=str, default='MNIST', help='Dataset to use (MNIST or CIFAR10).')\n",
    "parser.add_argument('--save_model', type=bool, default=True, help='Save model after training.')\n",
    "parser.add_argument('--wandb', default=\"online\", type=str, choices=[\"online\", \"disabled\"] , help=\"whether to track with weights and biases or not\")\n",
    "parser.add_argument('--heads', type=int, default=4, help='Number of heads for attention mechanism.')\n",
    "parser.add_argument('--noise_scheduler', type=str, default='cosine', choices=[\"linear\", \"cosine\"], help='Noise scheduler type.')\n",
    "parser.add_argument('--lr_scheduler', type=str, default='none', choices=[\"none\", \"warmup_linear\"], help='Learning rate scheduler type.')\n",
    "parser.add_argument('--seed', default=1, type=int, help=\"seed for reproducibility\")\n",
    "parser.add_argument('--fid', default=True, type=bool, help=\"Calculate FID for 10000 images after training\")\n",
    "parser.add_argument('--calculate_fid_25', default=True, type=bool, help=\"Calculate FID for 2500 images after every fid_epoch_modulo (e.g. 25th) epoch\")\n",
    "parser.add_argument('--fid_epoch_modulo', default=50, type=int, help=\"Which epochs to run fid\")\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Parse arguments\n",
    "    #args = parser.parse_args()\n",
    "    args = args # Redefine argument parsing when running in notebook\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    set_training_seed(args.seed)\n",
    "\n",
    "    # Unpack arguments\n",
    "    T = args.T\n",
    "    batch_size = args.batch_size\n",
    "    num_epochs = args.num_epochs\n",
    "    lr = args.lr\n",
    "    dataset = args.dataset\n",
    "    save_model = args.save_model\n",
    "    heads = args.heads\n",
    "    noise_scheduler = args.noise_scheduler\n",
    "    lr_scheduler = args.lr_scheduler\n",
    "    calculate_fid = args.fid\n",
    "    calculate_fid_25 = args.calculate_fid_25\n",
    "    fid_epoch_modulo = args.fid_epoch_modulo\n",
    "\n",
    "\n",
    "    # Scheduler parameters\n",
    "    warm_up_epochs = 2\n",
    "    if dataset == 'MNIST':\n",
    "        lr_decay_epochs = [20, 40, 60] \n",
    "    elif dataset == 'CIFAR10':\n",
    "        lr_decay_epochs = [200, 400, 500]\n",
    "\n",
    "\n",
    "    save_dir = \"./saved_models\"  # Directory to save the trained model\n",
    "\n",
    "    # Set number of input channels\n",
    "    num_input_channels = 1 if dataset == 'MNIST' else 3\n",
    "\n",
    "    # Set mode for Weights and Biases\n",
    "    mode_for_wandb = args.wandb\n",
    "    run_name = f\"{dataset}_bs_{batch_size}_Nscheduler_{noise_scheduler}_heads_{heads}_LRs_{lr_scheduler}_seed_{args.seed}\"\n",
    "\n",
    "    # Initialize Weights and Biases\n",
    "    wandb.init(project='ddpm', entity='dl_ddpm', mode=mode_for_wandb, name=run_name)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                      \"mps\" if torch.backends.mps.is_available() else \n",
    "                      \"cpu\")\n",
    "    print(f\"Using Device: {device}\")\n",
    "\n",
    "    # Dataloader\n",
    "    train_loader, _ = get_dataloader(dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize components \n",
    "    diffusion = Diffusion(T=T, beta_min=10e-5, beta_max=0.02, schedule=noise_scheduler, device=device) \n",
    "\n",
    "    time_embedding = SinusoidalPositionEmbeddings(total_time_steps=T, time_emb_dims=128, time_emb_dims_exp=512).to(device)\n",
    "\n",
    "    model = UNet(input_channels=num_input_channels, \n",
    "                 resolutions=[64, 128, 256, 512], \n",
    "                 time_emb_dims=512, \n",
    "                 dropout=0.1, \n",
    "                 use_attention=[False, True, False], \n",
    "                 heads=heads).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    if lr_scheduler == \"warmup_linear\":\n",
    "        scheduler = WarmUpPiecewiseConstantSchedule(optimizer=optimizer, steps_per_epoch=len(train_loader), base_lr=args.lr, \n",
    "                                                    lr_decay_ratio=0.2, lr_decay_epochs=lr_decay_epochs, warmup_epochs=warm_up_epochs)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    if calculate_fid_25:\n",
    "        inception_model = get_inception_model()\n",
    "        real_activations = get_real_image_activations(inception_classifier=inception_model, data=dataset, num_images=2500)\n",
    "    fid_epochs = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_ddpm_epoch(model, diffusion, time_embedding, train_loader, epoch, device, optimizer, scheduler)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            # Generate samples \n",
    "            samples = sample_ddpm(model, diffusion, time_embedding, device, num_samples=2, dataset=dataset) \n",
    "            wandb.log({\"Generated Samples\": [wandb.Image(sample, caption=f\"Epoch {epoch}\") for sample in samples]})\n",
    "\n",
    "        if calculate_fid_25:\n",
    "            if epoch == 1:\n",
    "                samples = sample_ddpm(model, diffusion, time_embedding, device, num_samples=2500, dataset=dataset)\n",
    "                fid_epochs.append(calculate_fid_generated_samples(generated_samples=samples, \n",
    "                                                                  real_image_activations=real_activations, \n",
    "                                                                  inception_classifier=inception_model,\n",
    "                                                                  data = dataset,\n",
    "                                                                  num_images = 2500, \n",
    "                                                                  device = device))\n",
    "                wandb.log({\"FID\": fid_epochs[-1]})\n",
    "            if epoch % fid_epoch_modulo == 0:\n",
    "                samples = sample_ddpm(model, diffusion, time_embedding, device, num_samples=2500, dataset=dataset)\n",
    "                fid_epochs.append(calculate_fid_generated_samples(generated_samples=samples, \n",
    "                                                                  real_image_activations=real_activations, \n",
    "                                                                  inception_classifier=inception_model,\n",
    "                                                                  data = dataset, \n",
    "                                                                  num_images = 2500, \n",
    "                                                                  device = device))\n",
    "                wandb.log({\"FID\": fid_epochs[-1]})\n",
    "    print(\"FID_epochs: \", fid_epochs)\n",
    "    if save_model:\n",
    "        final_save_path = f\"{save_dir}/ddpm_{dataset}_{noise_scheduler}_heads_{heads}_LRs_{lr_scheduler}_seed{args.seed}.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            \"embedding_state_dict\": time_embedding.state_dict(),\n",
    "        }, final_save_path)\n",
    "\n",
    "        print(f\"Model and embedding saved at: {final_save_path}\")\n",
    "\n",
    "    if calculate_fid:\n",
    "        # Calculate FID\n",
    "        samples = sample_ddpm(model, diffusion, time_embedding, device, num_samples=10000, dataset=dataset)\n",
    "        print(\"Samples generated successfully!\")\n",
    "        print(\"Shape samples: \", np.shape(samples))\n",
    "\n",
    "        fid = full_fid(samples, data = dataset, num_images = 10000)\n",
    "        wandb.log({\"full_FID\": fid})\n",
    "        print(\"Fid: \", fid)    \n",
    "        # Finish Weights and Biases run\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples using the script\n",
    "Since it is not feasible to run all our experiments in a notebook we provide a small example of how our setup runs. In reality we ran the main script with different arguments corresponding to our specific experiments. \n",
    "1) An example of MNIST - we only train for 5 epochs in our real experiments we run 75\n",
    "2) An example of CIFAR10 - we only train for 5 epochs in the notebook, but in our real experiments we run 600 epochs\n",
    "3) An example of FID and IS calculation based on a saved model - In our experiments we sample 10K generated images, but here we only sample 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MNIST Example ###\n",
    "from types import SimpleNamespace\n",
    "import sys\n",
    "args = SimpleNamespace(\n",
    "    T=1000,\n",
    "    batch_size=128,\n",
    "    num_epochs=5, #normally 75\n",
    "    lr=2e-4,\n",
    "    dataset=\"MNIST\",\n",
    "    save_model=False,\n",
    "    wandb=\"disabled\",\n",
    "    heads=1, # This is changed for the different models\n",
    "    noise_scheduler=\"linear\", # This is changed to \"cosine\" for models using a cosine noise schedule\n",
    "    lr_scheduler=\"none\", # This is changed to \"warmup_linear\" when using the custom lr_scheduler\n",
    "    seed=1,\n",
    "    fid=False,\n",
    "    calculate_fid_25=False,\n",
    "    fid_epoch_modulo=50\n",
    ")\n",
    "\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CIFAR10 Example ###\n",
    "args = SimpleNamespace(\n",
    "    T=1000,\n",
    "    batch_size=128,\n",
    "    num_epochs=5, # Normally 600\n",
    "    lr=2e-4,\n",
    "    dataset=\"CIFAR10\",\n",
    "    save_model=False,\n",
    "    wandb=\"disabled\",\n",
    "    heads=1,\n",
    "    noise_scheduler=\"linear\",\n",
    "    lr_scheduler=\"none\",\n",
    "    seed=1,\n",
    "    fid=False,\n",
    "    calculate_fid_25=False,\n",
    "    fid_epoch_modulo=50\n",
    ")\n",
    "\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example of FID + IS for trained CIFAR model ###\n",
    "\n",
    "set_training_seed(7)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                    \"mps\" if torch.backends.mps.is_available() else \n",
    "                    \"cpu\")\n",
    "print(f\"Using Device: {device}\")\n",
    "\n",
    "# Step 1: Initialize the Sinusoidal Embeddings\n",
    "time_embedding = SinusoidalPositionEmbeddings(total_time_steps=1000, time_emb_dims=128, time_emb_dims_exp=512).to(device)\n",
    "\n",
    "# Step 2: Initialize the U-Net and Diffusion\n",
    "unet = UNet(input_channels=1, resolutions=[64, 128, 256, 512], time_emb_dims=512, dropout=0.1, use_attention=[False, True, False], heads=4).to(device)\n",
    "diffusion = Diffusion(T=1000, beta_min=10e-5, beta_max=0.02, schedule='cosine', device=device)\n",
    "\n",
    "# Step 3: Load the trained model\n",
    "model_path = \"ddpm_CIFAR10_cosine_heads_4_LRs_warmup_linear_seed7.pth\" # insert model path\n",
    "print(\"Model_path: \", model_path)\n",
    "saved = torch.load(model_path, map_location=device)\n",
    "\n",
    "unet.load_state_dict(saved[\"model_state_dict\"])\n",
    "time_embedding.load_state_dict(saved[\"embedding_state_dict\"])\n",
    "\n",
    "\n",
    "# Step 4: Generate samples\n",
    "samples = sample_ddpm(unet, diffusion, time_embedding, device, num_samples=10000, dataset='CIFAR10')\n",
    "print(\"Samples generated successfully!\")\n",
    "\n",
    "fid = full_fid(samples, data = \"CIFAR10\", num_images = 10000)\n",
    "print(\"Fid: \", fid)\n",
    "\n",
    "# Calculate Inception Score\n",
    "mean_is, std_is = calculate_inception_score(samples, \"CIFAR10\", device)\n",
    "print(f\"Inception Score: {mean_is}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pixel_distribution(dataloader):\n",
    "    \"\"\"\n",
    "    Compute mean and std for each pixel position in the dataset.\n",
    "    Args:\n",
    "        dataloader: DataLoader containing the dataset.\n",
    "    Returns:\n",
    "        mean: Pixel-wise mean\n",
    "        std: Pixel-wise std\n",
    "    \"\"\"\n",
    "    pixel_sum = 0\n",
    "    pixel_sum_squared = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    for images, _ in dataloader:\n",
    "        pixel_sum += images.sum(dim=0)  # Sum across batch\n",
    "        pixel_sum_squared += (images ** 2).sum(dim=0)\n",
    "        num_samples += images.size(0)  # Number of samples in batch\n",
    "\n",
    "    mean = pixel_sum / num_samples\n",
    "    std = torch.sqrt(pixel_sum_squared / num_samples - mean ** 2)\n",
    "    return mean, std\n",
    "\n",
    "def generate_images(mean, std, num_images=10):\n",
    "    \"\"\"\n",
    "    Generate images by sampling from the learned distribution.\n",
    "    Args:\n",
    "        mean: Pixel-wise mean (height, width) or (channels, height, width).\n",
    "        std: Pixel-wise std deviation (same shape as mean).\n",
    "        num_images: Number of images to generate.\n",
    "    Returns:\n",
    "        Tensor of generated images.\n",
    "    \"\"\"\n",
    "    return torch.normal(mean=mean, std=std).unsqueeze(0).repeat(num_images, 1, 1, 1)\n",
    "\n",
    "# make new dataloaders without DDPM specific transforms\n",
    "def get_mnist_dataloader(batch_size=128):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    trainset = datasets.MNIST(root=\"./temp\", train=True, download=True, transform=transform)\n",
    "    return DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def get_cifar10_dataloader(batch_size=128):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    trainset = datasets.CIFAR10(root=\"./temp\", train=True, download=True, transform=transform)\n",
    "    return DataLoader(trainset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example for MNIST ###\n",
    "set_training_seed(7)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                    \"mps\" if torch.backends.mps.is_available() else \n",
    "                    \"cpu\")\n",
    "print(f\"Using Device: {device}\")\n",
    "\n",
    "# MNIST MPM (\"Upper Bound\")\n",
    "data = \"MNIST\"\n",
    "batch_size = 128\n",
    "print(\"MPM: \")\n",
    "dataloader = get_mnist_dataloader(batch_size=batch_size)\n",
    "mean, std = compute_pixel_distribution(dataloader)\n",
    "generated_images = generate_images(mean, std, num_images=10000)\n",
    "generated_images = generated_images.to(device)\n",
    "fid = full_fid(generated_samples=generated_images, data=data, num_images = 10000)\n",
    "print(\"Fid: \", fid)\n",
    "\n",
    "# MNIST REAL (\"Lower Bound\")\n",
    "print(\"REAL:\")\n",
    "inception_model = get_inception_model()\n",
    "real_activations = get_real_image_activations(inception_classifier=inception_model, data=data, num_images=20000)\n",
    "fid = calculate_fid(real_activations[:10000], real_activations[10000:])\n",
    "print(\"Fid: \", fid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DCGAN baseline fro CIFAR10 ###\n",
    "from dcgan_cifar import Discriminator, Generator\n",
    "\n",
    "num_gpu = 1 if torch.cuda.is_available() else 0\n",
    "\n",
    "D = Discriminator(ngpu=1).eval()\n",
    "G = Generator(ngpu=1).eval()\n",
    "\n",
    "D.load_state_dict(torch.load('netD_epoch_199.pth'))\n",
    "G.load_state_dict(torch.load('netG_epoch_199.pth'))\n",
    "if torch.cuda.is_available():\n",
    "    D = D.cuda()\n",
    "    G = G.cuda()\n",
    "\n",
    "\n",
    "def generate_fake_images(generator, num_samples=10000, latent_dim=100, device=device):\n",
    "    generator.eval()\n",
    "    z = torch.randn(num_samples, latent_dim, 1, 1, device=device)  # Random noise\n",
    "    with torch.no_grad():\n",
    "        fake_images = generator(z)  # Shape: [num_samples, 1, 28, 28]\n",
    "    return fake_images\n",
    "\n",
    "\n",
    "\n",
    "gen_img = generate_fake_images(G, num_samples=10000, latent_dim=100, device=device)\n",
    "print(np.shape(gen_img))\n",
    "print(\"MODEL: DC GAN\")\n",
    "fid = full_fid(gen_img, data=\"CIFAR10\" ,num_images=10000)\n",
    "print(\"FID:\", fid)\n",
    "\n",
    "mean_is, std_is = calculate_inception_score(samples=gen_img, dataset='CIFAR10', device=device, batch_size=32, splits=10)\n",
    "print(\"IS: \", mean_is)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See plots_for_report notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
