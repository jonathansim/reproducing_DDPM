{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keNDLgjq7qaF"
      },
      "source": [
        "## Functions for the setup ##\n",
        "## NB. Due to technical issues, this notebook will be run in a few days!!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-fK74b7s7qaG"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST, CIFAR10\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import numpy as np\n",
        "from scipy.linalg import fractional_matrix_power\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms, datasets\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix59DM4Q7qaG"
      },
      "source": [
        "### UNET ###\n",
        "The following code block includes the Sinusoidal positional embedding framework as well as the U-net architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BAYHt3dB7qaG"
      },
      "outputs": [],
      "source": [
        "### Positional Encoding ###\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    '''\n",
        "    From spmallick on GitHub\n",
        "\n",
        "    - chose to use this over own implementation as it is more efficient (due to precomputing the embeddings, allowing for faster training)\n",
        "    '''\n",
        "    def __init__(self, total_time_steps=1000, time_emb_dims=128, time_emb_dims_exp=512):\n",
        "        super().__init__()\n",
        "\n",
        "        half_dim = time_emb_dims // 2\n",
        "\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
        "\n",
        "        ts = torch.arange(total_time_steps, dtype=torch.float32)\n",
        "\n",
        "        emb = torch.unsqueeze(ts, dim=-1) * torch.unsqueeze(emb, dim=0)\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "\n",
        "        self.time_blocks = nn.Sequential(\n",
        "            nn.Embedding.from_pretrained(emb),\n",
        "            nn.Linear(in_features=time_emb_dims, out_features=time_emb_dims_exp),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(in_features=time_emb_dims_exp, out_features=time_emb_dims_exp),\n",
        "        )\n",
        "\n",
        "    def forward(self, time):\n",
        "        return self.time_blocks(time)\n",
        "\n",
        "\n",
        "### SUBMODULES ###\n",
        "# Block for self attention mechanism\n",
        "class SelfAttentionBlock(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dropout=0.0, dim_scale = 0.5):\n",
        "        '''\n",
        "        Attention Block with flexible dimension adjustment.\n",
        "\n",
        "        Args:\n",
        "            dim: Number of input channels.\n",
        "            num_heads: Number of attention heads.\n",
        "            dim_scale: Scale factor for input dimensions.\n",
        "                        - Use dim_scale=0.5 for downblock.\n",
        "                        - Use dim_scale=1 for upblock.\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.scaled_dim = int(dim * dim_scale)\n",
        "        self.norm = nn.LayerNorm(self.scaled_dim)\n",
        "        self.mhsa = nn.MultiheadAttention(self.scaled_dim, num_heads=heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # Reshape for multihead attention\n",
        "        x_flat = x.view(B, C, -1).transpose(1, 2) # shape (B, H*W, C)\n",
        "\n",
        "        # normalize and apply multihead attention\n",
        "        x_norm = self.norm(x_flat)\n",
        "        attention_out = self.mhsa(x_norm, x_norm, x_norm)[0]\n",
        "\n",
        "        # Add residual connection\n",
        "        x = x_flat + attention_out # we do this to ensure that no information from the input is lost (no matter how the attention might have changed the input)\n",
        "\n",
        "        # Reshape back to original shape\n",
        "        x = x.transpose(1, 2).view(B, C, H, W)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Block for downsampling part of U-net\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dims, dropout=0.1, use_attention=False, heads=4):\n",
        "        super().__init__()\n",
        "        # First conv\n",
        "        self.norm1 = nn.GroupNorm(8, in_channels)\n",
        "        self.activation1 = nn.SiLU()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels//2, kernel_size=3, padding=1)\n",
        "\n",
        "        # Second conv\n",
        "        self.norm2 = nn.GroupNorm(8, out_channels//2)\n",
        "        self.activation2 = nn.SiLU()\n",
        "        self.conv2 = nn.Conv2d(out_channels//2, out_channels//2, kernel_size=3, padding=1)\n",
        "\n",
        "        # Dropout, downsample, timestep embedding, and self-attention\n",
        "        self.dropout = nn.Dropout2d(p=dropout)\n",
        "        self.downsample = nn.Conv2d(out_channels//2, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "        self.time_proj = nn.Linear(time_emb_dims, out_channels//2)\n",
        "        if use_attention:\n",
        "            self.attention = SelfAttentionBlock(dim=out_channels, heads=heads, dim_scale=0.5)\n",
        "        else:\n",
        "            self.attention = nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        # Feed time embedding through linear layer\n",
        "        time_emb_proj = self.time_proj(time_emb).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        # Normalize and apply activation\n",
        "        x = self.activation1(self.norm1(x))\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # Add timestep embedding\n",
        "        x += self.activation1(time_emb_proj)\n",
        "\n",
        "        # Normalize and apply activation\n",
        "        x = self.activation2(self.norm2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        # Apply self-attention\n",
        "        x = self.attention(x)\n",
        "\n",
        "        skip = x  # Save for skip connection\n",
        "        x = self.downsample(x)\n",
        "        return x, skip\n",
        "\n",
        "\n",
        "# Block for bottleneck part of U-net\n",
        "class BottleneckBlock(nn.Module):\n",
        "    def __init__(self, channels, time_emb_dims, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # First conv\n",
        "        self.norm1 = nn.GroupNorm(8, channels)\n",
        "        self.activation1 = nn.SiLU()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "\n",
        "        # Second conv\n",
        "        self.norm2 = nn.GroupNorm(8, channels)\n",
        "        self.activation2 = nn.SiLU()\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "\n",
        "        self.dropout = nn.Dropout2d(p=dropout)\n",
        "        self.time_proj = nn.Linear(time_emb_dims, channels)\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        # Feed time embedding through linear layer\n",
        "        time_emb_proj = self.time_proj(time_emb).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        # Normalize and apply activation\n",
        "        x = self.activation1(self.norm1(x))\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # Add timestep embedding\n",
        "        x += self.activation1(time_emb_proj)\n",
        "\n",
        "        # Normalize and apply activation\n",
        "        x = self.activation2(self.norm2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Block for upsampling part of U-net\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dims, dropout=0.1, use_attention=False, heads=4):\n",
        "        super().__init__()\n",
        "        # First conv\n",
        "        self.norm1 = nn.GroupNorm(8, in_channels)\n",
        "        self.activation1 = nn.SiLU()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        # Second conv\n",
        "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
        "        self.activation2 = nn.SiLU()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        self.upsample = nn.ConvTranspose2d(out_channels*2, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "        self.time_proj = nn.Linear(time_emb_dims, out_channels)\n",
        "        self.dropout = nn.Dropout2d(p=dropout)\n",
        "\n",
        "        if use_attention:\n",
        "            self.attention = SelfAttentionBlock(dim=out_channels, heads=heads, dim_scale=1)\n",
        "        else:\n",
        "            self.attention = nn.Identity()\n",
        "\n",
        "    def forward(self, x, skip, time_emb):\n",
        "        # Upsample\n",
        "        x = self.upsample(x)\n",
        "\n",
        "        # Feed time embedding through linear layer\n",
        "        time_emb_proj = self.time_proj(time_emb).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        # Resize if spatial dimensions mismatch\n",
        "        if x.shape[-2:] != skip.shape[-2:]:\n",
        "            x = F.interpolate(x, size=skip.shape[-2:], mode='nearest')\n",
        "\n",
        "        # Add skip connection\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "\n",
        "        # Normalize and apply activation\n",
        "        x = self.activation1(self.norm1(x))\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # Add timestep embedding\n",
        "        x += self.activation1(time_emb_proj)\n",
        "\n",
        "        # Normalize and apply activation\n",
        "        x = self.activation2(self.norm2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        # Apply self-attention\n",
        "        x = self.attention(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "### U-NET MODEL ###\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, input_channels=1, resolutions=[64, 128, 256, 512], time_emb_dims=512, dropout=0.1, use_attention=[False, True, False], heads=4):\n",
        "        \"\"\"\n",
        "        U-Net implementation for DDPM\n",
        "        Args:\n",
        "            input_channels: Number of input channels (e.g., 1 for MNIST).\n",
        "            base_channels: Number of channels in the first convolution layer.\n",
        "            num_resolutions: Number of downsampling/upsampling blocks.\n",
        "            time_emb_dims: Dimensionality of timestep embeddings.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.base_channels = resolutions[0]\n",
        "\n",
        "\n",
        "        # Input layer\n",
        "        self.input_conv = nn.Conv2d(input_channels, self.base_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        # Downsampling blocks\n",
        "        self.down_blocks = nn.ModuleList([\n",
        "            DownBlock(\n",
        "                in_channels=resolutions[i],\n",
        "                out_channels=resolutions[i+1],\n",
        "                time_emb_dims=time_emb_dims,\n",
        "                dropout=dropout,\n",
        "                use_attention=use_attention[i],\n",
        "                heads=heads\n",
        "            )\n",
        "            for i in range(len(resolutions) - 1)\n",
        "        ])\n",
        "\n",
        "\n",
        "        # Bottleneck block\n",
        "        self.bottleneck = BottleneckBlock(channels=resolutions[-1], time_emb_dims=time_emb_dims, dropout=dropout)\n",
        "\n",
        "        # Upsampling blocks\n",
        "        self.up_blocks = nn.ModuleList([\n",
        "            UpBlock(\n",
        "                in_channels=resolutions[i + 1],\n",
        "                out_channels=resolutions[i],\n",
        "                time_emb_dims=time_emb_dims,\n",
        "                dropout=dropout,\n",
        "                use_attention=use_attention[i],\n",
        "                heads=heads\n",
        "            )\n",
        "            for i in reversed(range(len(resolutions) - 1))\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.output_conv = nn.Conv2d(self.base_channels, input_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        \"\"\"\n",
        "        Forward pass of the U-Net.\n",
        "        Args:\n",
        "            x: Input image tensor of shape (batch_size, input_channels, height, width).\n",
        "            time_emb: Timestep embedding tensor of shape (batch_size, time_emb_dims).\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, input_channels, height, width).\n",
        "        \"\"\"\n",
        "\n",
        "        # Input conv\n",
        "        x = self.input_conv(x)\n",
        "\n",
        "        # Downsampling path\n",
        "        skips = []\n",
        "        for block in self.down_blocks:\n",
        "            x, skip = block(x, time_emb)\n",
        "            skips.append(skip)\n",
        "\n",
        "        #print(f\"Skips: {len(skips)}\")\n",
        "        # Bottleneck block\n",
        "        x = self.bottleneck(x, time_emb)\n",
        "\n",
        "        # Upsampling path\n",
        "        for block, skip in zip(self.up_blocks, reversed(skips)):\n",
        "            x = block(x, skip, time_emb)\n",
        "\n",
        "        # Output conv\n",
        "        x = self.output_conv(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwoqD5oH7qaH"
      },
      "source": [
        "### Diffusion Model\n",
        "The DDPM setup consists of the UNET defined above, a Diffusion class implementing forward and reverse processes, a training function implementing alg. 1 and a sampling func. implementing alg. 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JQOth03v7qaH"
      },
      "outputs": [],
      "source": [
        "class Diffusion:\n",
        "    def __init__(self, T=1000, beta_min=10e-5, beta_max=0.02, schedule='linear', device='cpu'):\n",
        "        \"\"\"\n",
        "        Initialize the diffusion process.\n",
        "        Args:\n",
        "            T: Total number of timesteps.\n",
        "            beta_min: Minimum value of beta in the noise schedule.\n",
        "            beta_max: Maximum value of beta in the noise schedule.\n",
        "            schedule: Type of noise schedule ('linear', 'cosine', etc.).\n",
        "            device: Device to use for computations.\n",
        "        \"\"\"\n",
        "        self.T = T\n",
        "        self.beta_min = beta_min\n",
        "        self.beta_max = beta_max\n",
        "        self.schedule = schedule\n",
        "        self.device = device\n",
        "\n",
        "        # Noise schedule parameters\n",
        "        self.beta = None\n",
        "        self.alpha = None\n",
        "        self.alpha_bar = None\n",
        "\n",
        "        # Precompute schedule\n",
        "        self.get_noise_schedule()\n",
        "\n",
        "    def get_noise_schedule(self):\n",
        "        \"\"\"\n",
        "        Precompute the noise schedule.\n",
        "        \"\"\"\n",
        "        if self.schedule == 'linear':\n",
        "            self.beta = torch.linspace(self.beta_min, self.beta_max, self.T).to(self.device)\n",
        "            self.alpha = (1 - self.beta).to(self.device)\n",
        "            self.alpha_bar = self.alpha.cumprod(dim=0).to(self.device)\n",
        "\n",
        "        elif self.schedule == 'cosine':\n",
        "            steps = torch.linspace(0, torch.pi, self.T).to(self.device)\n",
        "            self.beta = ((torch.cos(steps) + 1) * 0.5 * (self.beta_max - self.beta_min) + self.beta_min).flip(0)\n",
        "            self.alpha = (1 - self.beta).to(self.device)\n",
        "            self.alpha_bar = self.alpha.cumprod(dim=0).to(self.device)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown schedule type: {self.schedule}\")\n",
        "\n",
        "    def forward_diffusion(self, x0, t):\n",
        "        \"\"\"\n",
        "        Perform the forward diffusion process.\n",
        "        Args:\n",
        "            x0: Original data (e.g., images).\n",
        "            t: Timesteps (tensor of integers).\n",
        "        Returns:\n",
        "            xt: Noisy data at timestep t.\n",
        "            epsilon: The noise added.\n",
        "        \"\"\"\n",
        "        eps = torch.randn_like(x0)\n",
        "\n",
        "        sqrt_alpha_bar_t = self.alpha_bar[t].sqrt().view(-1, 1, 1, 1)\n",
        "        sqrt_one_minus_alpha_bar_t = (1 - self.alpha_bar[t]).sqrt().view(-1, 1, 1, 1)\n",
        "\n",
        "        xt = sqrt_alpha_bar_t * x0 + sqrt_one_minus_alpha_bar_t * eps\n",
        "\n",
        "        return xt, eps\n",
        "\n",
        "    def reverse_diffusion(self, xt, t, model, time_embedding):\n",
        "        \"\"\"\n",
        "        Perform the reverse diffusion process.\n",
        "        Args:\n",
        "            xt: Noisy data at timestep t.\n",
        "            t: Timesteps for batch (tensor of integers).\n",
        "            model: Model used for reverse diffusion.\n",
        "            time_embedding: Time embedding object.\n",
        "        Returns:\n",
        "            xr: Reconstructed data at timestep t.\n",
        "        \"\"\"\n",
        "        z = torch.where((t > 1).view(-1, 1, 1, 1), torch.randn_like(xt), torch.zeros_like(xt))\n",
        "\n",
        "        # extract time embedding\n",
        "        time_emb = time_embedding(t)\n",
        "\n",
        "        # predict noise\n",
        "        eps_theta = model(xt, time_emb)\n",
        "\n",
        "        sqrt_alpha_t = self.alpha[t].sqrt().view(-1, 1, 1, 1)  # we need to reshape the tensor to match the shape of xt (same goes for the other tensors)\n",
        "        beta_t = self.beta[t].view(-1, 1, 1, 1)\n",
        "        sqrt_one_minus_alpha_bar_t = (1 - self.alpha_bar[t]).sqrt().view(-1, 1, 1, 1)\n",
        "        sigma_t = beta_t.sqrt()\n",
        "\n",
        "        xt_minus_one = 1. / sqrt_alpha_t * (xt - (beta_t / sqrt_one_minus_alpha_bar_t) * eps_theta) + sigma_t * z\n",
        "\n",
        "        return xt_minus_one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cHIB16dg7qaH"
      },
      "outputs": [],
      "source": [
        "### Function for training DDPM ###\n",
        "def train_ddpm_epoch(model: object, diffusion: object, time_embedding: object, train_loader: object, epoch: int, device: str, optimizer: object, lr_scheduler: object):\n",
        "    \"\"\"\n",
        "    This function implements Algorithm 1 from the paper, specifically it trains the U-Net model for one epoch.\n",
        "    Args:\n",
        "        model: U-Net model.\n",
        "        diffusion: Diffusion object.\n",
        "        time_embedding: Time embedding object.\n",
        "        train_loader: DataLoader for training data.\n",
        "        epoch: Current epoch.\n",
        "        device: Device to use (e.g., 'cuda' or 'cpu').\n",
        "        optimizer: Optimizer for training.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Training epoch {epoch}...\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (x0, _) in enumerate(train_loader):\n",
        "        x0 = x0.to(device)\n",
        "\n",
        "        # Sample random timestep\n",
        "        t = torch.randint(0, diffusion.T, (x0.size(0),), device=device)\n",
        "\n",
        "        # Get sinusoidal time embedding\n",
        "        time_emb = time_embedding(t)\n",
        "\n",
        "        # Forward diffusion\n",
        "        xt, epsilon = diffusion.forward_diffusion(x0, t)\n",
        "\n",
        "        # Predict noise\n",
        "        epsilon_pred = model(xt, time_emb)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = F.mse_loss(epsilon_pred, epsilon)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Use learning rate scheduler\n",
        "        if lr_scheduler:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        # Log loss\n",
        "        wandb.log({\"loss\": loss.item(), \"epoch\": epoch, \"lr\": optimizer.param_groups[0]['lr']})\n",
        "\n",
        "        if epoch > 1:\n",
        "            wandb.log({\"loss after 1st epoch\": loss.item()})\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, batch {i}: loss={loss.item()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gRuR5smU7qaH"
      },
      "outputs": [],
      "source": [
        "### Function to sample from DDPM ###\n",
        "\n",
        "def sample_ddpm(model: object, diffusion: object, time_embedding: object, device: str, num_samples: int = 16, dataset: str = 'MNIST'):\n",
        "    \"\"\"\n",
        "    This function implements Algorithm 2 from the paper, specifically it samples from the U-Net model.\n",
        "    Args:\n",
        "        model: U-Net model.\n",
        "        diffusion: Diffusion object.\n",
        "        time_embedding: Time embedding object.\n",
        "        device: Device to use (e.g., 'cuda' or 'cpu').\n",
        "        num_samples: Number of samples to generate.\n",
        "        dataset: Dataset to use ('MNIST' or 'CIFAR10').\n",
        "    Returns:\n",
        "        samples: Generated samples.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Sampling {num_samples} samples...\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # 1. Initialize samples from standard gaussian distribution\n",
        "        if dataset == 'MNIST':\n",
        "            x = torch.randn((num_samples, 1, 28, 28), device=device)\n",
        "        elif dataset == 'CIFAR10':\n",
        "            x = torch.randn((num_samples, 3, 32, 32), device=device)\n",
        "\n",
        "        # 2. iterative reverse diffusion\n",
        "        for t in reversed(range(1, diffusion.T + 1)):\n",
        "            # prepare timestep tensor\n",
        "            t_tensor = torch.full((num_samples,), t - 1, device=device, dtype=torch.long)\n",
        "\n",
        "            # Perform reverse diffusion step\n",
        "            x = diffusion.reverse_diffusion(x, t_tensor, model, time_embedding)\n",
        "\n",
        "        # 3. Return samples\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LBVhXHg7qaH"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TW5PxxjO7qaH"
      },
      "outputs": [],
      "source": [
        "### Dataloader ###\n",
        "def get_dataloader(dataset = \"MNIST\", batch_size = 128):\n",
        "    transforms = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                # No flips in MNIST as it disturbes the interpretation of numbers?\n",
        "                #torchvision.transforms.Resize((32, 32), interpolation=torchvision.transforms.InterpolationMode.BICUBIC), # TODO: Resize to 32 x 32 for unet implementation?\n",
        "                torchvision.transforms.Normalize((0.5,), (0.5,))  # Correpsonds to scaling between [-1, 1] --> (x - 0.5)/0.5\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    transforms_cifar10_train = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                torchvision.transforms.RandomHorizontalFlip(),\n",
        "                #torchvision.transforms.Resize((32, 32), interpolation=torchvision.transforms.InterpolationMode.BICUBIC), # TODO: Match paper or resize to 28 x 28 for unet implementation\n",
        "                torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Scaling between [-1, 1]\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    transforms_cifar10_test = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                #torchvision.transforms.Resize((32, 32), interpolation=torchvision.transforms.InterpolationMode.BICUBIC), # TODO: Match paper or resize to 28 x 28 for unet implementation\n",
        "                torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Scaling between [-1, 1]\n",
        "            ]\n",
        "        )\n",
        "    if dataset == \"MNIST\":\n",
        "        trainset = MNIST(\"./temp/\", train=True, download=True, transform= transforms)\n",
        "        testset = MNIST(\"./temp/\", train=False, download=True, transform= transforms)\n",
        "    elif dataset == \"CIFAR10\":\n",
        "        trainset = CIFAR10(\"./temp/\", train=True, download=True, transform= transforms_cifar10_train)\n",
        "        testset = CIFAR10(\"./temp/\", train=False, download=True, transform= transforms_cifar10_test)\n",
        "\n",
        "\n",
        "    train_dataloader = DataLoader(trainset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
        "    test_dataloader = DataLoader(testset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
        "    return train_dataloader, test_dataloader\n",
        "\n",
        "\n",
        "def get_dataloader_evaluation(dataset = \"MNIST\", batch_size = 128):\n",
        "    transforms_mnist = torchvision.transforms.Compose(\n",
        "        [\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
        "            torchvision.transforms.Resize((299, 299)),  # Resize for InceptionV3\n",
        "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  #normalize for InceptionV3\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    transforms_cifar10 = torchvision.transforms.Compose(\n",
        "          [\n",
        "              torchvision.transforms.Resize((299, 299)),  # Resize for InceptionV3\n",
        "              torchvision.transforms.ToTensor(),\n",
        "              torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # CIFAR normalization\n",
        "          ])\n",
        "\n",
        "\n",
        "    if dataset == \"MNIST\":\n",
        "        trainset = MNIST(\"./temp/\", train=True, download=True, transform= transforms_mnist)\n",
        "    elif dataset == \"CIFAR10\":\n",
        "        trainset = CIFAR10(\"./temp/\", train=True, download=True, transform= transforms_cifar10)\n",
        "\n",
        "\n",
        "    train_dataloader = DataLoader(trainset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
        "    return train_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI3vWLpX7qaH"
      },
      "source": [
        "### Custom learning rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MbNfhxeq7qaI"
      },
      "outputs": [],
      "source": [
        "### Custom Learning Rate Scheduler ###\n",
        "class WarmUpPiecewiseConstantSchedule(_LRScheduler):\n",
        "    '''\n",
        "    This class implements a learning rate scheduler that combines a warm-up phase with a piecewise constant decay.\n",
        "    That is, the learning rate is increased linearly from 0 to the base learning rate during the warm-up phase, and then\n",
        "    decreased by a factor of decay_ratio at each epoch in decay_epochs (similarly to MultiStepLR).\n",
        "    Note: can maybe be done better, but needed it done quickly...\n",
        "\n",
        "    Note: I implemented this class as part of my bachelor's thesis (jonathansim).\n",
        "    '''\n",
        "    def __init__(self, optimizer, steps_per_epoch, base_lr, lr_decay_ratio, lr_decay_epochs, warmup_epochs, last_epoch=-1):\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "        self.base_lr = base_lr\n",
        "        self.decay_ratio = lr_decay_ratio\n",
        "        self.decay_epochs = lr_decay_epochs\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.decay_steps = [e * steps_per_epoch for e in lr_decay_epochs]  # Convert epochs to steps\n",
        "        super(WarmUpPiecewiseConstantSchedule, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        # Calculate the current step\n",
        "        lr_step = self.last_epoch\n",
        "        lr_epoch = lr_step / self.steps_per_epoch\n",
        "        learning_rate = self.base_lr\n",
        "\n",
        "        # Warm-Up Phase\n",
        "        if lr_epoch < self.warmup_epochs:\n",
        "            learning_rate = self.base_lr * lr_step / (self.warmup_epochs * self.steps_per_epoch)\n",
        "        else:\n",
        "            # Piecewise Constant Decay Phase\n",
        "            for i, start_step in enumerate(self.decay_steps):\n",
        "                if lr_step >= start_step:\n",
        "                    learning_rate = self.base_lr * (self.decay_ratio ** (i + 1))\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        return [learning_rate for _ in self.base_lrs]\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "        self.last_epoch = epoch\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1MUWK8hb7qaI"
      },
      "outputs": [],
      "source": [
        "### Function for setting seed ###\n",
        "def set_training_seed(seed):\n",
        "    # Function to set the different seeds\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSTGL-Rp7qaI"
      },
      "source": [
        "### Evaluation functions ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KsrIprS57qaI"
      },
      "outputs": [],
      "source": [
        "### Frechet Inception Distance ###\n",
        "def calculate_fid(feat1, feat2):\n",
        "    '''\n",
        "    calculate fid between two sets of images\n",
        "    feat1: real images - feature vector obtained from inception model (or alternative model)\n",
        "    feat2: generated images - feature vector obtained from inception model (or alternative model)\n",
        "    return: fid score\n",
        "    '''\n",
        "    # calculate mean and covariance of features\n",
        "    mu1, sigma1 = np.mean(feat1, axis=0), np.cov(feat1, rowvar=False)\n",
        "    mu2, sigma2 = np.mean(feat2, axis=0), np.cov(feat2, rowvar=False)\n",
        "    print(\"Mean1: \", np.shape(mu1))\n",
        "    print(\"Mean2: \", np.shape(mu2))\n",
        "    # sum squared difference between means\n",
        "    diff = np.sum((mu1 - mu2)**2)\n",
        "    print(\"diff: \", diff)\n",
        "    # calculate \"geometric mean\" of covariance matrices\n",
        "    covmean = fractional_matrix_power(sigma1.dot(sigma2), 0.5)\n",
        "    # check for imaginary numbers\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = np.real(covmean)\n",
        "\n",
        "    fid = diff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
        "    return fid\n",
        "\n",
        "\n",
        "def compute_activations(samples, model, batched = False, batch_size = 100):\n",
        "    \"\"\"\n",
        "    Compute activations from samples based on model.\n",
        "    Args:\n",
        "        samples: Tensor of shape (num_samples, channels, height, width)\n",
        "        model: Pre-trained model to extract features\n",
        "    Returns:\n",
        "        activations: Feature activations for input images\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    samples = samples.to(next(model.parameters()).device)  # Move to the same device as the model\n",
        "\n",
        "    if batched:\n",
        "        activations = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, samples.size(0), batch_size):\n",
        "                batch = samples[i:i+batch_size]\n",
        "                batch_activations = model(batch).cpu().numpy()\n",
        "                activations.append(batch_activations)\n",
        "        return np.concatenate(activations, axis=0)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            activations = model(samples)\n",
        "        return activations.cpu().numpy()\n",
        "\n",
        "\n",
        "def get_images_in_batches(dataloader, num_images=None):\n",
        "    \"\"\"\n",
        "    Fetch e.g. CIFAR-10 images in smaller batches without concatenating them.\n",
        "    Args:\n",
        "        dataloader: PyTorch DataLoader providing the dataset.\n",
        "        num_images: Number of images to fetch.\n",
        "    Yields:\n",
        "        Batches of real images from the dataset.\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    for images, _ in dataloader:\n",
        "        count += len(images)\n",
        "        if num_images and count >= num_images:\n",
        "            yield images\n",
        "            break\n",
        "\n",
        "        # Yield images batch-by-batch\n",
        "        yield images\n",
        "\n",
        "        # Free memory\n",
        "        del images\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def full_fid(generated_samples, data, num_images = 10000):\n",
        "    \"\"\"\n",
        "    Computes the Fréchet Inception Distance (FID) between generated images and real images.\n",
        "    Args:\n",
        "        generated_samples (torch.Tensor): Tensor of generated samples\n",
        "            with shape (num_samples, channels, height, width).\n",
        "        data (str): MNIST or CIFAR10\n",
        "        num_images (int, optional): Number of real images to use for FID computation.\n",
        "            Defaults to 10,000.\n",
        "    Returns:\n",
        "        float: FID score between the generated and real images.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load pre-trained InceptionV3\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    inception_classifier = models.inception_v3(weights='DEFAULT', transform_input=False)\n",
        "    inception_classifier.fc = torch.nn.Identity()  # remove classification layer to extract features\n",
        "    inception_classifier = inception_classifier.to(device)\n",
        "\n",
        "    # Get train samples\n",
        "    dataloader = get_dataloader_evaluation(dataset = data, batch_size = 100)\n",
        "    print(\"DATA: \", data)\n",
        "    # Get activations for real images\n",
        "    real_image_activations = []\n",
        "    for batch in get_images_in_batches(dataloader, num_images=num_images):\n",
        "        activation = compute_activations(batch, inception_classifier)\n",
        "        real_image_activations.extend(activation)\n",
        "    real_image_activations = real_image_activations[:num_images]\n",
        "\n",
        "    # Get activations of generated images\n",
        "    min_val = generated_samples.min()\n",
        "    max_val = generated_samples.max()\n",
        "    resized_samples = (generated_samples - min_val) / (max_val - min_val) # rescale pixel values to [0,1]\n",
        "    if data == \"MNIST\":\n",
        "        resized_samples = resized_samples.repeat(1, 3, 1, 1)\n",
        "    resized_samples = torchvision.transforms.functional.resize(resized_samples, size = (299, 299)) # resize for inception\n",
        "    resized_samples = torchvision.transforms.functional.normalize(resized_samples,\n",
        "        mean=torch.tensor([0.485, 0.456, 0.406]).view(1, -1, 1, 1).to(device),\n",
        "        std=torch.tensor([0.229, 0.224, 0.225]).view(1, -1, 1, 1).to(device)\n",
        "    )\n",
        "    generated_image_activations = compute_activations(resized_samples, inception_classifier, batched = True)\n",
        "    print(np.shape(generated_image_activations))\n",
        "    print(np.shape(real_image_activations))\n",
        "    fid = calculate_fid(real_image_activations, generated_image_activations)\n",
        "\n",
        "    return fid\n",
        "\n",
        "# full_fid splitted up\n",
        "def get_inception_model():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    inception_classifier = models.inception_v3(weights='DEFAULT', transform_input=False)\n",
        "    inception_classifier.fc = torch.nn.Identity()  # remove classification layer to extract features\n",
        "    inception_classifier = inception_classifier.to(device)\n",
        "    return inception_classifier\n",
        "\n",
        "\n",
        "def get_real_image_activations(inception_classifier, data, num_images = 10000):\n",
        "    # Get train samples\n",
        "    dataloader = get_dataloader_evaluation(dataset = data, batch_size = 100)\n",
        "    print(\"DATA: \", data)\n",
        "    # Get activations for real images\n",
        "    real_image_activations = []\n",
        "    for batch in get_images_in_batches(dataloader, num_images=num_images):\n",
        "        activation = compute_activations(batch, inception_classifier)\n",
        "        real_image_activations.extend(activation)\n",
        "    real_image_activations = real_image_activations[:num_images]\n",
        "    return real_image_activations\n",
        "\n",
        "\n",
        "def calculate_fid_generated_samples(generated_samples, real_image_activations, inception_classifier, device, data, num_images = 10000):\n",
        "    # Get activations of generated images\n",
        "    min_val = generated_samples.min()\n",
        "    max_val = generated_samples.max()\n",
        "    resized_samples = (generated_samples - min_val) / (max_val - min_val) # rescale pixel values to [0,1]\n",
        "    if data == \"MNIST\":\n",
        "        resized_samples = resized_samples.repeat(1, 3, 1, 1)\n",
        "    resized_samples = torchvision.transforms.functional.resize(resized_samples, size = (299, 299)) # resize for inception\n",
        "    resized_samples = torchvision.transforms.functional.normalize(resized_samples,\n",
        "        mean=torch.tensor([0.485, 0.456, 0.406]).view(1, -1, 1, 1).to(device),\n",
        "        std=torch.tensor([0.229, 0.224, 0.225]).view(1, -1, 1, 1).to(device)\n",
        "    )\n",
        "    generated_image_activations = compute_activations(resized_samples, inception_classifier, batched = True)\n",
        "    print(np.shape(generated_image_activations))\n",
        "    print(np.shape(real_image_activations))\n",
        "    fid = calculate_fid(real_image_activations, generated_image_activations)\n",
        "    return fid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bgv1eLCA7qaI"
      },
      "outputs": [],
      "source": [
        "### Inception Score ###\n",
        "def calculate_inception_score(samples, dataset, device, batch_size=32, splits=10):\n",
        "    \"\"\"\n",
        "    Calculates the Inception Score for generated samples.\n",
        "    Inspired by: https://github.com/sbarratt/inception-score-pytorch/blob/master/inception_score.py\n",
        "    and paper: https://papers.nips.cc/paper_files/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf\n",
        "\n",
        "    Args:\n",
        "        samples (torch.Tensor): Generated samples with shape (num_samples, channels, height, width).\n",
        "        device (torch.device): Device to perform calculations on\n",
        "        dataset (str): One of the two datasets\n",
        "        batch_size (int): Batch size for processing samples.\n",
        "        splits (int): Number of splits for calculating the mean and standard deviation of the score.\n",
        "\n",
        "    Returns:\n",
        "        float, float: Mean and standard deviation of the Inception Score.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load pretrained InceptionV3 model\n",
        "    inception_model = models.inception_v3(pretrained=True)  # keep the final classification layer\n",
        "    inception_model = inception_model.to(device)\n",
        "\n",
        "    inception_model.eval()\n",
        "    samples = samples.to(device)\n",
        "    num_samples = samples.size(0)\n",
        "\n",
        "    # Placeholder for storing predictions\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch = samples[i:i + batch_size]\n",
        "            if dataset == \"MNIST\":\n",
        "                batch = batch.repeat(1, 3, 1, 1)  # repeat channels for Inception when using MNIST\n",
        "            batch = transforms.functional.resize(batch, size=(299, 299))  # resize for Inception\n",
        "            batch = transforms.functional.normalize(batch,\n",
        "                mean=torch.tensor([0.485, 0.456, 0.406]).to(device),\n",
        "                std=torch.tensor([0.229, 0.224, 0.225]).to(device),\n",
        "            )\n",
        "            pred = inception_model(batch)\n",
        "            pred = F.softmax(pred, dim=1)\n",
        "            preds.append(pred.cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "\n",
        "    # Compute IS\n",
        "    scores = []\n",
        "    split_size = preds.shape[0] // splits\n",
        "    for i in range(splits):\n",
        "        part = preds[i * split_size: (i + 1) * split_size] # probability of labels conditioned on image\n",
        "        p_y = np.mean(part, axis=0)\n",
        "        kl_div = part * (np.log(part) - np.log(p_y[None, :]))\n",
        "        kl_div = np.sum(kl_div, axis=1)\n",
        "        scores.append(np.exp(np.mean(kl_div)))\n",
        "\n",
        "    return np.mean(scores), np.std(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrSW_g5r7qaI"
      },
      "source": [
        "## Training of Models ##\n",
        "Since we run various experiments with two different datasets MNIST and CIFAR10 and different parameters, we made a general pipeline for training and evaluating our models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "o9A_488D7qaI"
      },
      "outputs": [],
      "source": [
        "### Main training script ###\n",
        "\n",
        "## Argument parser\n",
        "parser = argparse.ArgumentParser(description='Train (and sample from) DDPM framework.')\n",
        "\n",
        "## Add arguments\n",
        "parser.add_argument('--T', type=int, default=1000, help='Total timesteps.')\n",
        "parser.add_argument('--batch_size', type=int, default=128, help='Batch size.')\n",
        "parser.add_argument('--num_epochs', type=int, default=5, help='Number of epochs.')\n",
        "parser.add_argument('--lr', type=float, default=2e-4, help='Learning rate.')\n",
        "parser.add_argument('--dataset', type=str, default='MNIST', help='Dataset to use (MNIST or CIFAR10).')\n",
        "parser.add_argument('--save_model', type=bool, default=True, help='Save model after training.')\n",
        "parser.add_argument('--wandb', default=\"online\", type=str, choices=[\"online\", \"disabled\"] , help=\"whether to track with weights and biases or not\")\n",
        "parser.add_argument('--heads', type=int, default=4, help='Number of heads for attention mechanism.')\n",
        "parser.add_argument('--noise_scheduler', type=str, default='cosine', choices=[\"linear\", \"cosine\"], help='Noise scheduler type.')\n",
        "parser.add_argument('--lr_scheduler', type=str, default='none', choices=[\"none\", \"warmup_linear\"], help='Learning rate scheduler type.')\n",
        "parser.add_argument('--seed', default=1, type=int, help=\"seed for reproducibility\")\n",
        "parser.add_argument('--fid', default=True, type=bool, help=\"Calculate FID for 10000 images after training\")\n",
        "parser.add_argument('--calculate_fid_25', default=True, type=bool, help=\"Calculate FID for 2500 images after every fid_epoch_modulo (e.g. 25th) epoch\")\n",
        "parser.add_argument('--fid_epoch_modulo', default=50, type=int, help=\"Which epochs to run fid\")\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # Parse arguments\n",
        "    #args = parser.parse_args()\n",
        "    args = args # Redefine argument parsing when running in notebook\n",
        "\n",
        "    # Set seed for reproducibility\n",
        "    set_training_seed(args.seed)\n",
        "\n",
        "    # Unpack arguments\n",
        "    T = args.T\n",
        "    batch_size = args.batch_size\n",
        "    num_epochs = args.num_epochs\n",
        "    lr = args.lr\n",
        "    dataset = args.dataset\n",
        "    save_model = args.save_model\n",
        "    heads = args.heads\n",
        "    noise_scheduler = args.noise_scheduler\n",
        "    lr_scheduler = args.lr_scheduler\n",
        "    calculate_fid = args.fid\n",
        "    calculate_fid_25 = args.calculate_fid_25\n",
        "    fid_epoch_modulo = args.fid_epoch_modulo\n",
        "\n",
        "\n",
        "    # Scheduler parameters\n",
        "    warm_up_epochs = 2\n",
        "    if dataset == 'MNIST':\n",
        "        lr_decay_epochs = [20, 40, 60]\n",
        "    elif dataset == 'CIFAR10':\n",
        "        lr_decay_epochs = [200, 400, 500]\n",
        "\n",
        "\n",
        "    save_dir = \"./saved_models\"  # Directory to save the trained model\n",
        "\n",
        "    # Set number of input channels\n",
        "    num_input_channels = 1 if dataset == 'MNIST' else 3\n",
        "\n",
        "    # Set mode for Weights and Biases\n",
        "    mode_for_wandb = args.wandb\n",
        "    run_name = f\"{dataset}_bs_{batch_size}_Nscheduler_{noise_scheduler}_heads_{heads}_LRs_{lr_scheduler}_seed_{args.seed}\"\n",
        "\n",
        "    # Initialize Weights and Biases\n",
        "    wandb.init(project='ddpm', entity='dl_ddpm', mode=mode_for_wandb, name=run_name)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
        "                      \"mps\" if torch.backends.mps.is_available() else\n",
        "                      \"cpu\")\n",
        "    print(f\"Using Device: {device}\")\n",
        "\n",
        "    # Dataloader\n",
        "    train_loader, _ = get_dataloader(dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize components\n",
        "    diffusion = Diffusion(T=T, beta_min=10e-5, beta_max=0.02, schedule=noise_scheduler, device=device)\n",
        "\n",
        "    time_embedding = SinusoidalPositionEmbeddings(total_time_steps=T, time_emb_dims=128, time_emb_dims_exp=512).to(device)\n",
        "\n",
        "    model = UNet(input_channels=num_input_channels,\n",
        "                 resolutions=[64, 128, 256, 512],\n",
        "                 time_emb_dims=512,\n",
        "                 dropout=0.1,\n",
        "                 use_attention=[False, True, False],\n",
        "                 heads=heads).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    if lr_scheduler == \"warmup_linear\":\n",
        "        scheduler = WarmUpPiecewiseConstantSchedule(optimizer=optimizer, steps_per_epoch=len(train_loader), base_lr=args.lr,\n",
        "                                                    lr_decay_ratio=0.2, lr_decay_epochs=lr_decay_epochs, warmup_epochs=warm_up_epochs)\n",
        "    else:\n",
        "        scheduler = None\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    if calculate_fid_25:\n",
        "        inception_model = get_inception_model()\n",
        "        real_activations = get_real_image_activations(inception_classifier=inception_model, data=dataset, num_images=2500)\n",
        "    fid_epochs = []\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_ddpm_epoch(model, diffusion, time_embedding, train_loader, epoch, device, optimizer, scheduler)\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            # Generate samples\n",
        "            samples = sample_ddpm(model, diffusion, time_embedding, device, num_samples=2, dataset=dataset)\n",
        "            wandb.log({\"Generated Samples\": [wandb.Image(sample, caption=f\"Epoch {epoch}\") for sample in samples]})\n",
        "\n",
        "        if calculate_fid_25:\n",
        "            if epoch == 1:\n",
        "                samples = sample_ddpm(model, diffusion, time_embedding, device, num_samples=2500, dataset=dataset)\n",
        "                fid_epochs.append(calculate_fid_generated_samples(generated_samples=samples,\n",
        "                                                                  real_image_activations=real_activations,\n",
        "                                                                  inception_classifier=inception_model,\n",
        "                                                                  data = dataset,\n",
        "                                                                  num_images = 2500,\n",
        "                                                                  device = device))\n",
        "                wandb.log({\"FID\": fid_epochs[-1]})\n",
        "            if epoch % fid_epoch_modulo == 0:\n",
        "                samples = sample_ddpm(model, diffusion, time_embedding, device, num_samples=2500, dataset=dataset)\n",
        "                fid_epochs.append(calculate_fid_generated_samples(generated_samples=samples,\n",
        "                                                                  real_image_activations=real_activations,\n",
        "                                                                  inception_classifier=inception_model,\n",
        "                                                                  data = dataset,\n",
        "                                                                  num_images = 2500,\n",
        "                                                                  device = device))\n",
        "                wandb.log({\"FID\": fid_epochs[-1]})\n",
        "    print(\"FID_epochs: \", fid_epochs)\n",
        "    if save_model:\n",
        "        final_save_path = f\"{save_dir}/ddpm_{dataset}_{noise_scheduler}_heads_{heads}_LRs_{lr_scheduler}_seed{args.seed}.pth\"\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            \"embedding_state_dict\": time_embedding.state_dict(),\n",
        "        }, final_save_path)\n",
        "\n",
        "        print(f\"Model and embedding saved at: {final_save_path}\")\n",
        "\n",
        "    if calculate_fid:\n",
        "        # Calculate FID\n",
        "        samples = sample_ddpm(model, diffusion, time_embedding, device, num_samples=10000, dataset=dataset)\n",
        "        print(\"Samples generated successfully!\")\n",
        "        print(\"Shape samples: \", np.shape(samples))\n",
        "\n",
        "        fid = full_fid(samples, data = dataset, num_images = 10000)\n",
        "        wandb.log({\"full_FID\": fid})\n",
        "        print(\"Fid: \", fid)\n",
        "        # Finish Weights and Biases run\n",
        "        wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMseNIUI7qaI"
      },
      "source": [
        "## Examples using the script\n",
        "Since it is not feasible to run all our experiments in a notebook we provide a small example of how our setup runs. In reality we ran the main script with different arguments corresponding to our specific experiments.\n",
        "1) An example of MNIST - we only train for 5 epochs in our real experiments we run 75\n",
        "2) An example of CIFAR10 - we only train for 5 epochs in the notebook, but in our real experiments we run 600 epochs\n",
        "3) An example of FID and IS calculation based on a saved model - In our experiments we sample 10K generated images, but here we only sample 2500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zFf9_axv7qaI",
        "outputId": "272831a3-e31e-445f-f706-99979740806e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Device: cuda\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./temp/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./temp/MNIST/raw/train-images-idx3-ubyte.gz to ./temp/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./temp/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 347kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./temp/MNIST/raw/train-labels-idx1-ubyte.gz to ./temp/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./temp/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.77MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./temp/MNIST/raw/t10k-images-idx3-ubyte.gz to ./temp/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./temp/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.91MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./temp/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./temp/MNIST/raw\n",
            "\n",
            "Training epoch 1...\n",
            "Epoch 1, batch 0: loss=1.0248407125473022\n",
            "Epoch 1, batch 100: loss=0.09949691593647003\n",
            "Epoch 1, batch 200: loss=0.06893058121204376\n",
            "Epoch 1, batch 300: loss=0.04778468608856201\n",
            "Epoch 1, batch 400: loss=0.05255862697958946\n",
            "Training epoch 2...\n",
            "Epoch 2, batch 0: loss=0.05860847234725952\n",
            "Epoch 2, batch 100: loss=0.03983432799577713\n",
            "Epoch 2, batch 200: loss=0.035100847482681274\n",
            "Epoch 2, batch 300: loss=0.04526690021157265\n",
            "Epoch 2, batch 400: loss=0.03606978803873062\n",
            "Training epoch 3...\n",
            "Epoch 3, batch 0: loss=0.03434152528643608\n",
            "Epoch 3, batch 100: loss=0.032479044049978256\n",
            "Epoch 3, batch 200: loss=0.04145487770438194\n",
            "Epoch 3, batch 300: loss=0.03720972314476967\n",
            "Epoch 3, batch 400: loss=0.029410317540168762\n",
            "Training epoch 4...\n",
            "Epoch 4, batch 0: loss=0.0346800722181797\n",
            "Epoch 4, batch 100: loss=0.032416850328445435\n",
            "Epoch 4, batch 200: loss=0.03310177102684975\n",
            "Epoch 4, batch 300: loss=0.03253283351659775\n",
            "Epoch 4, batch 400: loss=0.033776480704545975\n",
            "Training epoch 5...\n",
            "Epoch 5, batch 0: loss=0.0302775539457798\n",
            "Epoch 5, batch 100: loss=0.030553089454770088\n",
            "Epoch 5, batch 200: loss=0.03257891163229942\n",
            "Epoch 5, batch 300: loss=0.029009146615862846\n",
            "Epoch 5, batch 400: loss=0.02578519657254219\n",
            "Sampling 2 samples...\n",
            "FID_epochs:  []\n"
          ]
        }
      ],
      "source": [
        "### MNIST Example ###\n",
        "from types import SimpleNamespace\n",
        "import sys\n",
        "args = SimpleNamespace(\n",
        "    T=1000,\n",
        "    batch_size=128,\n",
        "    num_epochs=5, #normally 75\n",
        "    lr=2e-4,\n",
        "    dataset=\"MNIST\",\n",
        "    save_model=False,\n",
        "    wandb=\"disabled\",\n",
        "    heads=1, # This is changed for the different models\n",
        "    noise_scheduler=\"linear\", # This is changed to \"cosine\" for models using a cosine noise schedule\n",
        "    lr_scheduler=\"none\", # This is changed to \"warmup_linear\" when using the custom lr_scheduler\n",
        "    seed=1,\n",
        "    fid=False,\n",
        "    calculate_fid_25=False,\n",
        "    fid_epoch_modulo=50\n",
        ")\n",
        "\n",
        "main(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "ChQpiFW57qaI",
        "outputId": "005d5685-1848-460a-d543-91a706983a33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Device: cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./temp/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 30.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./temp/cifar-10-python.tar.gz to ./temp/\n",
            "Files already downloaded and verified\n",
            "Training epoch 1...\n",
            "Epoch 1, batch 0: loss=1.0528299808502197\n",
            "Epoch 1, batch 100: loss=0.15049698948860168\n",
            "Epoch 1, batch 200: loss=0.10594990104436874\n",
            "Epoch 1, batch 300: loss=0.08235377073287964\n",
            "Training epoch 2...\n",
            "Epoch 2, batch 0: loss=0.07530054450035095\n",
            "Epoch 2, batch 100: loss=0.08824054896831512\n",
            "Epoch 2, batch 200: loss=0.07016375660896301\n",
            "Epoch 2, batch 300: loss=0.06308020651340485\n",
            "Training epoch 3...\n",
            "Epoch 3, batch 0: loss=0.06016600877046585\n",
            "Epoch 3, batch 100: loss=0.04957980662584305\n",
            "Epoch 3, batch 200: loss=0.06536881625652313\n",
            "Epoch 3, batch 300: loss=0.05608316510915756\n",
            "Training epoch 4...\n",
            "Epoch 4, batch 0: loss=0.05260009691119194\n",
            "Epoch 4, batch 100: loss=0.05747954174876213\n",
            "Epoch 4, batch 200: loss=0.05371551960706711\n",
            "Epoch 4, batch 300: loss=0.06030559539794922\n",
            "Training epoch 5...\n",
            "Epoch 5, batch 0: loss=0.0474037230014801\n",
            "Epoch 5, batch 100: loss=0.0632072240114212\n",
            "Epoch 5, batch 200: loss=0.06077512726187706\n",
            "Epoch 5, batch 300: loss=0.03385879844427109\n",
            "Sampling 2 samples...\n",
            "FID_epochs:  []\n"
          ]
        }
      ],
      "source": [
        "### CIFAR10 Example ###\n",
        "args = SimpleNamespace(\n",
        "    T=1000,\n",
        "    batch_size=128,\n",
        "    num_epochs=5, # Normally 600\n",
        "    lr=2e-4,\n",
        "    dataset=\"CIFAR10\",\n",
        "    save_model=False,\n",
        "    wandb=\"disabled\",\n",
        "    heads=1,\n",
        "    noise_scheduler=\"linear\",\n",
        "    lr_scheduler=\"none\",\n",
        "    seed=1,\n",
        "    fid=False,\n",
        "    calculate_fid_25=False,\n",
        "    fid_epoch_modulo=50\n",
        ")\n",
        "\n",
        "main(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "_Joh4RI07qaI",
        "outputId": "cd858fdf-9d79-40ee-9f2f-c875efd96457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Device: mps\n",
            "Model_path:  trained_models/ddpm_CIFAR10_cosine_heads_4_LRs_warmup_linear_seed7.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ql/h53w_qjs60509nsf4jbr01c00000gn/T/ipykernel_95023/799747291.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  saved = torch.load(model_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampling 100 samples...\n",
            "Samples generated successfully!\n",
            "Files already downloaded and verified\n",
            "DATA:  CIFAR10\n"
          ]
        },
        {
          "ename": "NotImplementedError",
          "evalue": "The operator 'aten::_upsample_bilinear2d_aa.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[34], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m samples \u001b[38;5;241m=\u001b[39m sample_ddpm(unet, diffusion, time_embedding, device, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIFAR10\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# usually 10K images, but here lowered for faster calculation\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSamples generated successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m fid \u001b[38;5;241m=\u001b[39m \u001b[43mfull_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCIFAR10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_images\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# usually 10K images, but here lowered for faster calculation\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFid: \u001b[39m\u001b[38;5;124m\"\u001b[39m, fid)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Calculate Inception Score\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[10], line 112\u001b[0m, in \u001b[0;36mfull_fid\u001b[0;34m(generated_samples, data, num_images)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMNIST\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    111\u001b[0m     resized_samples \u001b[38;5;241m=\u001b[39m resized_samples\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m resized_samples \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresized_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m299\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m299\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# resize for inception\u001b[39;00m\n\u001b[1;32m    113\u001b[0m resized_samples \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(resized_samples, \n\u001b[1;32m    114\u001b[0m     mean\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m])\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[1;32m    115\u001b[0m     std\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    116\u001b[0m )\n\u001b[1;32m    117\u001b[0m generated_image_activations \u001b[38;5;241m=\u001b[39m compute_activations(resized_samples, inception_classifier, batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/Documents/Deep Learning/reproducing_DDPM/env/lib/python3.10/site-packages/torchvision/transforms/functional.py:479\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[0;32m--> 479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/Deep Learning/reproducing_DDPM/env/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:467\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# Define align_corners to avoid warnings\u001b[39;00m\n\u001b[1;32m    465\u001b[0m align_corners \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out_dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8:\n\u001b[1;32m    470\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/Deep Learning/reproducing_DDPM/env/lib/python3.10/site-packages/torch/nn/functional.py:4565\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4563\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m antialias:\n\u001b[0;32m-> 4565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upsample_bilinear2d_aa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4566\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\n\u001b[1;32m   4567\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4568\u001b[0m \u001b[38;5;66;03m# Two levels are necessary to prevent TorchScript from touching\u001b[39;00m\n\u001b[1;32m   4569\u001b[0m \u001b[38;5;66;03m# are_deterministic_algorithms_enabled.\u001b[39;00m\n\u001b[1;32m   4570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::_upsample_bilinear2d_aa.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
          ]
        }
      ],
      "source": [
        "### Example of FID + IS for trained CIFAR model ###\n",
        "\n",
        "set_training_seed(7)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
        "                    \"mps\" if torch.backends.mps.is_available() else\n",
        "                    \"cpu\")\n",
        "print(f\"Using Device: {device}\")\n",
        "\n",
        "# Step 1: Initialize the Sinusoidal Embeddings\n",
        "time_embedding = SinusoidalPositionEmbeddings(total_time_steps=1000, time_emb_dims=128, time_emb_dims_exp=512).to(device)\n",
        "\n",
        "# Step 2: Initialize the U-Net and Diffusion\n",
        "unet = UNet(input_channels=3, resolutions=[64, 128, 256, 512], time_emb_dims=512, dropout=0.1, use_attention=[False, True, False], heads=4).to(device)\n",
        "diffusion = Diffusion(T=1000, beta_min=10e-5, beta_max=0.02, schedule='cosine', device=device)\n",
        "\n",
        "# Step 3: Load the trained model\n",
        "model_path = \"trained_models/ddpm_CIFAR10_cosine_heads_4_LRs_warmup_linear_seed7.pth\" # insert model path for model we wish to evaluate\n",
        "print(\"Model_path: \", model_path)\n",
        "saved = torch.load(model_path, map_location=device)\n",
        "\n",
        "unet.load_state_dict(saved[\"model_state_dict\"])\n",
        "time_embedding.load_state_dict(saved[\"embedding_state_dict\"])\n",
        "\n",
        "\n",
        "# Step 4: Generate samples\n",
        "samples = sample_ddpm(unet, diffusion, time_embedding, device, num_samples=100, dataset='CIFAR10') # usually 10K images, but here lowered for faster calculation\n",
        "print(\"Samples generated successfully!\")\n",
        "\n",
        "fid = full_fid(samples, data = \"CIFAR10\", num_images = 100) # usually 10K images, but here lowered for faster calculation\n",
        "print(\"Fid: \", fid)\n",
        "\n",
        "# Calculate Inception Score\n",
        "mean_is, std_is = calculate_inception_score(samples, \"CIFAR10\", device)\n",
        "print(f\"Inception Score: {mean_is}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c_Eya3-7qaJ"
      },
      "source": [
        "### Baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4Ie5Mp8Q7qaJ"
      },
      "outputs": [],
      "source": [
        "def compute_pixel_distribution(dataloader):\n",
        "    \"\"\"\n",
        "    Compute mean and std for each pixel position in the dataset.\n",
        "    Args:\n",
        "        dataloader: DataLoader containing the dataset.\n",
        "    Returns:\n",
        "        mean: Pixel-wise mean\n",
        "        std: Pixel-wise std\n",
        "    \"\"\"\n",
        "    pixel_sum = 0\n",
        "    pixel_sum_squared = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    for images, _ in dataloader:\n",
        "        pixel_sum += images.sum(dim=0)  # Sum across batch\n",
        "        pixel_sum_squared += (images ** 2).sum(dim=0)\n",
        "        num_samples += images.size(0)  # Number of samples in batch\n",
        "\n",
        "    mean = pixel_sum / num_samples\n",
        "    std = torch.sqrt(pixel_sum_squared / num_samples - mean ** 2)\n",
        "    return mean, std\n",
        "\n",
        "def generate_images(mean, std, num_images=10):\n",
        "    \"\"\"\n",
        "    Generate images by sampling from the learned distribution.\n",
        "    Args:\n",
        "        mean: Pixel-wise mean (height, width) or (channels, height, width).\n",
        "        std: Pixel-wise std deviation (same shape as mean).\n",
        "        num_images: Number of images to generate.\n",
        "    Returns:\n",
        "        Tensor of generated images.\n",
        "    \"\"\"\n",
        "    return torch.normal(mean=mean, std=std).unsqueeze(0).repeat(num_images, 1, 1, 1)\n",
        "\n",
        "# make new dataloaders without DDPM specific transforms\n",
        "def get_mnist_dataloader(batch_size=128):\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    trainset = datasets.MNIST(root=\"./temp\", train=True, download=True, transform=transform)\n",
        "    return DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "def get_cifar10_dataloader(batch_size=128):\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    trainset = datasets.CIFAR10(root=\"./temp\", train=True, download=True, transform=transform)\n",
        "    return DataLoader(trainset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoHFmPun7qaJ",
        "outputId": "ac9e48e6-393d-4874-edca-9f4250f14d91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Device: cuda\n",
            "MPM: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:00<00:00, 162MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA:  MNIST\n",
            "(2500, 2048)\n",
            "(2500, 2048)\n",
            "Mean1:  (2048,)\n",
            "Mean2:  (2048,)\n",
            "diff:  311.23703\n",
            "Fid:  421.49151427009207\n",
            "REAL:\n",
            "DATA:  MNIST\n",
            "Mean1:  (2048,)\n",
            "Mean2:  (2048,)\n",
            "diff:  0.08329368\n",
            "Fid:  4.9017019593237485\n"
          ]
        }
      ],
      "source": [
        "### Example for MNIST ###\n",
        "set_training_seed(7)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
        "                    \"mps\" if torch.backends.mps.is_available() else\n",
        "                    \"cpu\")\n",
        "print(f\"Using Device: {device}\")\n",
        "\n",
        "# MNIST MPM (\"Upper Bound\") (again only evaluated with 2.5K images instead of 10K)\n",
        "data = \"MNIST\"\n",
        "batch_size = 128\n",
        "print(\"MPM: \")\n",
        "dataloader = get_mnist_dataloader(batch_size=batch_size)\n",
        "mean, std = compute_pixel_distribution(dataloader)\n",
        "generated_images = generate_images(mean, std, num_images=2500)\n",
        "generated_images = generated_images.to(device)\n",
        "fid = full_fid(generated_samples=generated_images, data=data, num_images = 2500)\n",
        "print(\"Fid: \", fid)\n",
        "\n",
        "# MNIST REAL (\"Lower Bound\") (again only evaluated with 2.5K images instead of 10K)\n",
        "print(\"REAL:\")\n",
        "inception_model = get_inception_model()\n",
        "real_activations = get_real_image_activations(inception_classifier=inception_model, data=data, num_images=5000)\n",
        "fid = calculate_fid(real_activations[:2500], real_activations[2500:])\n",
        "print(\"Fid: \", fid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBKK2rIG7qaJ"
      },
      "outputs": [],
      "source": [
        "### DCGAN baseline fro CIFAR10 ###\n",
        "from dcgan_cifar import Discriminator, Generator\n",
        "\n",
        "num_gpu = 1 if torch.cuda.is_available() else 0\n",
        "\n",
        "D = Discriminator(ngpu=1).eval()\n",
        "G = Generator(ngpu=1).eval()\n",
        "\n",
        "D.load_state_dict(torch.load('netD_epoch_199.pth'))\n",
        "G.load_state_dict(torch.load('netG_epoch_199.pth'))\n",
        "if torch.cuda.is_available():\n",
        "    D = D.cuda()\n",
        "    G = G.cuda()\n",
        "\n",
        "\n",
        "def generate_fake_images(generator, num_samples=10000, latent_dim=100, device=device):\n",
        "    generator.eval()\n",
        "    z = torch.randn(num_samples, latent_dim, 1, 1, device=device)  # Random noise\n",
        "    with torch.no_grad():\n",
        "        fake_images = generator(z)  # Shape: [num_samples, 1, 28, 28]\n",
        "    return fake_images\n",
        "\n",
        "\n",
        "\n",
        "gen_img = generate_fake_images(G, num_samples=10000, latent_dim=100, device=device)\n",
        "print(np.shape(gen_img))\n",
        "print(\"MODEL: DC GAN\")\n",
        "fid = full_fid(gen_img, data=\"CIFAR10\" ,num_images=10000)\n",
        "print(\"FID:\", fid)\n",
        "\n",
        "mean_is, std_is = calculate_inception_score(samples=gen_img, dataset='CIFAR10', device=device, batch_size=32, splits=10)\n",
        "print(\"IS: \", mean_is)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJUzjDxs7qaJ"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TY9eTwK7qaJ"
      },
      "source": [
        "See plots_for_report notebook"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
