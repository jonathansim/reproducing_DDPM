{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keNDLgjq7qaF"
   },
   "source": [
    "## Functions for the setup ##\n",
    "## NB.\n",
    "During the project, we have run everything on DTU's HPC using normal python scripts as found in the git repo. However, for this notebook, we had to run it locally. \n",
    "This unfortunately means that we have not had the computational resources to run certain cells (i.e. the FID computation for the model trained on CIFAR). \n",
    "\n",
    "Furthermore, this notebook does not train the models for the full epochs either - e.g., we normally train the CIFAR models for 600 epochs, but here we only train a few epochs. \n",
    "\n",
    "Moreover, not all model configurations (i.e. all configurations of number of attention heads, noise schedule and learning rate scheduler) are also not present in this notebook as this would be very space consuming. All of our trained models are stored in a drive. \n",
    "\n",
    "In the last part of the notebook, we showcase some visual samples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-fK74b7s7qaG"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "# import to fix issue where it crashes when run on MPS (needed because we cant run notebook on cuda)\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"]=\"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import numpy as np\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, datasets\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ix59DM4Q7qaG"
   },
   "source": [
    "### UNET ###\n",
    "The following code block includes the Sinusoidal positional embedding framework as well as the U-net architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "BAYHt3dB7qaG"
   },
   "outputs": [],
   "source": [
    "### Positional Encoding ###\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    '''\n",
    "    From spmallick on GitHub\n",
    "\n",
    "    - chose to use this over own implementation as it is more efficient (due to precomputing the embeddings, allowing for faster training)\n",
    "    '''\n",
    "    def __init__(self, total_time_steps=1000, time_emb_dims=128, time_emb_dims_exp=512):\n",
    "        super().__init__()\n",
    "\n",
    "        half_dim = time_emb_dims // 2\n",
    "\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "\n",
    "        ts = torch.arange(total_time_steps, dtype=torch.float32)\n",
    "\n",
    "        emb = torch.unsqueeze(ts, dim=-1) * torch.unsqueeze(emb, dim=0)\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "\n",
    "        self.time_blocks = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(emb),\n",
    "            nn.Linear(in_features=time_emb_dims, out_features=time_emb_dims_exp),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(in_features=time_emb_dims_exp, out_features=time_emb_dims_exp),\n",
    "        )\n",
    "\n",
    "    def forward(self, time):\n",
    "        return self.time_blocks(time)\n",
    "\n",
    "\n",
    "### SUBMODULES ###\n",
    "# Block for self attention mechanism\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dropout=0.0, dim_scale = 0.5):\n",
    "        '''\n",
    "        Attention Block with flexible dimension adjustment.\n",
    "\n",
    "        Args:\n",
    "            dim: Number of input channels.\n",
    "            num_heads: Number of attention heads.\n",
    "            dim_scale: Scale factor for input dimensions.\n",
    "                        - Use dim_scale=0.5 for downblock.\n",
    "                        - Use dim_scale=1 for upblock.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.scaled_dim = int(dim * dim_scale)\n",
    "        self.norm = nn.LayerNorm(self.scaled_dim)\n",
    "        self.mhsa = nn.MultiheadAttention(self.scaled_dim, num_heads=heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Reshape for multihead attention\n",
    "        x_flat = x.view(B, C, -1).transpose(1, 2) # shape (B, H*W, C)\n",
    "\n",
    "        # normalize and apply multihead attention\n",
    "        x_norm = self.norm(x_flat)\n",
    "        attention_out = self.mhsa(x_norm, x_norm, x_norm)[0]\n",
    "\n",
    "        # Add residual connection\n",
    "        x = x_flat + attention_out # we do this to ensure that no information from the input is lost (no matter how the attention might have changed the input)\n",
    "\n",
    "        # Reshape back to original shape\n",
    "        x = x.transpose(1, 2).view(B, C, H, W)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Block for downsampling part of U-net\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_emb_dims, dropout=0.1, use_attention=False, heads=4):\n",
    "        super().__init__()\n",
    "        # First conv\n",
    "        self.norm1 = nn.GroupNorm(8, in_channels)\n",
    "        self.activation1 = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels//2, kernel_size=3, padding=1)\n",
    "\n",
    "        # Second conv\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels//2)\n",
    "        self.activation2 = nn.SiLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels//2, out_channels//2, kernel_size=3, padding=1)\n",
    "\n",
    "        # Dropout, downsample, timestep embedding, and self-attention\n",
    "        self.dropout = nn.Dropout2d(p=dropout)\n",
    "        self.downsample = nn.Conv2d(out_channels//2, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.time_proj = nn.Linear(time_emb_dims, out_channels//2)\n",
    "        if use_attention:\n",
    "            self.attention = SelfAttentionBlock(dim=out_channels, heads=heads, dim_scale=0.5)\n",
    "        else:\n",
    "            self.attention = nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb):\n",
    "        # Feed time embedding through linear layer\n",
    "        time_emb_proj = self.time_proj(time_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # Normalize and apply activation\n",
    "        x = self.activation1(self.norm1(x))\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # Add timestep embedding\n",
    "        x += self.activation1(time_emb_proj)\n",
    "\n",
    "        # Normalize and apply activation\n",
    "        x = self.activation2(self.norm2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # Apply self-attention\n",
    "        x = self.attention(x)\n",
    "\n",
    "        skip = x  # Save for skip connection\n",
    "        x = self.downsample(x)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "# Block for bottleneck part of U-net\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, channels, time_emb_dims, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # First conv\n",
    "        self.norm1 = nn.GroupNorm(8, channels)\n",
    "        self.activation1 = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # Second conv\n",
    "        self.norm2 = nn.GroupNorm(8, channels)\n",
    "        self.activation2 = nn.SiLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dropout = nn.Dropout2d(p=dropout)\n",
    "        self.time_proj = nn.Linear(time_emb_dims, channels)\n",
    "\n",
    "    def forward(self, x, time_emb):\n",
    "        # Feed time embedding through linear layer\n",
    "        time_emb_proj = self.time_proj(time_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # Normalize and apply activation\n",
    "        x = self.activation1(self.norm1(x))\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # Add timestep embedding\n",
    "        x += self.activation1(time_emb_proj)\n",
    "\n",
    "        # Normalize and apply activation\n",
    "        x = self.activation2(self.norm2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Block for upsampling part of U-net\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_emb_dims, dropout=0.1, use_attention=False, heads=4):\n",
    "        super().__init__()\n",
    "        # First conv\n",
    "        self.norm1 = nn.GroupNorm(8, in_channels)\n",
    "        self.activation1 = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # Second conv\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.activation2 = nn.SiLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upsample = nn.ConvTranspose2d(out_channels*2, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.time_proj = nn.Linear(time_emb_dims, out_channels)\n",
    "        self.dropout = nn.Dropout2d(p=dropout)\n",
    "\n",
    "        if use_attention:\n",
    "            self.attention = SelfAttentionBlock(dim=out_channels, heads=heads, dim_scale=1)\n",
    "        else:\n",
    "            self.attention = nn.Identity()\n",
    "\n",
    "    def forward(self, x, skip, time_emb):\n",
    "        # Upsample\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        # Feed time embedding through linear layer\n",
    "        time_emb_proj = self.time_proj(time_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # Resize if spatial dimensions mismatch\n",
    "        if x.shape[-2:] != skip.shape[-2:]:\n",
    "            x = F.interpolate(x, size=skip.shape[-2:], mode='nearest')\n",
    "\n",
    "        # Add skip connection\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "\n",
    "        # Normalize and apply activation\n",
    "        x = self.activation1(self.norm1(x))\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # Add timestep embedding\n",
    "        x += self.activation1(time_emb_proj)\n",
    "\n",
    "        # Normalize and apply activation\n",
    "        x = self.activation2(self.norm2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # Apply self-attention\n",
    "        x = self.attention(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "### U-NET MODEL ###\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels=1, resolutions=[64, 128, 256, 512], time_emb_dims=512, dropout=0.1, use_attention=[False, True, False], heads=4):\n",
    "        \"\"\"\n",
    "        U-Net implementation for DDPM\n",
    "        Args:\n",
    "            input_channels: Number of input channels (e.g., 1 for MNIST).\n",
    "            base_channels: Number of channels in the first convolution layer.\n",
    "            num_resolutions: Number of downsampling/upsampling blocks.\n",
    "            time_emb_dims: Dimensionality of timestep embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_channels = resolutions[0]\n",
    "\n",
    "\n",
    "        # Input layer\n",
    "        self.input_conv = nn.Conv2d(input_channels, self.base_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # Downsampling blocks\n",
    "        self.down_blocks = nn.ModuleList([\n",
    "            DownBlock(\n",
    "                in_channels=resolutions[i],\n",
    "                out_channels=resolutions[i+1],\n",
    "                time_emb_dims=time_emb_dims,\n",
    "                dropout=dropout,\n",
    "                use_attention=use_attention[i],\n",
    "                heads=heads\n",
    "            )\n",
    "            for i in range(len(resolutions) - 1)\n",
    "        ])\n",
    "\n",
    "\n",
    "        # Bottleneck block\n",
    "        self.bottleneck = BottleneckBlock(channels=resolutions[-1], time_emb_dims=time_emb_dims, dropout=dropout)\n",
    "\n",
    "        # Upsampling blocks\n",
    "        self.up_blocks = nn.ModuleList([\n",
    "            UpBlock(\n",
    "                in_channels=resolutions[i + 1],\n",
    "                out_channels=resolutions[i],\n",
    "                time_emb_dims=time_emb_dims,\n",
    "                dropout=dropout,\n",
    "                use_attention=use_attention[i],\n",
    "                heads=heads\n",
    "            )\n",
    "            for i in reversed(range(len(resolutions) - 1))\n",
    "        ])\n",
    "\n",
    "        # Output layer\n",
    "        self.output_conv = nn.Conv2d(self.base_channels, input_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, time_emb):\n",
    "        \"\"\"\n",
    "        Forward pass of the U-Net.\n",
    "        Args:\n",
    "            x: Input image tensor of shape (batch_size, input_channels, height, width).\n",
    "            time_emb: Timestep embedding tensor of shape (batch_size, time_emb_dims).\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, input_channels, height, width).\n",
    "        \"\"\"\n",
    "\n",
    "        # Input conv\n",
    "        x = self.input_conv(x)\n",
    "\n",
    "        # Downsampling path\n",
    "        skips = []\n",
    "        for block in self.down_blocks:\n",
    "            x, skip = block(x, time_emb)\n",
    "            skips.append(skip)\n",
    "\n",
    "        #print(f\"Skips: {len(skips)}\")\n",
    "        # Bottleneck block\n",
    "        x = self.bottleneck(x, time_emb)\n",
    "\n",
    "        # Upsampling path\n",
    "        for block, skip in zip(self.up_blocks, reversed(skips)):\n",
    "            x = block(x, skip, time_emb)\n",
    "\n",
    "        # Output conv\n",
    "        x = self.output_conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwoqD5oH7qaH"
   },
   "source": [
    "### Diffusion Model\n",
    "The DDPM setup consists of the UNET defined above, a Diffusion class implementing forward and reverse processes, a training function implementing alg. 1 and a sampling func. implementing alg. 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JQOth03v7qaH"
   },
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(self, T=1000, beta_min=10e-5, beta_max=0.02, schedule='linear', device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the diffusion process.\n",
    "        Args:\n",
    "            T: Total number of timesteps.\n",
    "            beta_min: Minimum value of beta in the noise schedule.\n",
    "            beta_max: Maximum value of beta in the noise schedule.\n",
    "            schedule: Type of noise schedule ('linear', 'cosine', etc.).\n",
    "            device: Device to use for computations.\n",
    "        \"\"\"\n",
    "        self.T = T\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "        self.schedule = schedule\n",
    "        self.device = device\n",
    "\n",
    "        # Noise schedule parameters\n",
    "        self.beta = None\n",
    "        self.alpha = None\n",
    "        self.alpha_bar = None\n",
    "\n",
    "        # Precompute schedule\n",
    "        self.get_noise_schedule()\n",
    "\n",
    "    def get_noise_schedule(self):\n",
    "        \"\"\"\n",
    "        Precompute the noise schedule.\n",
    "        \"\"\"\n",
    "        if self.schedule == 'linear':\n",
    "            self.beta = torch.linspace(self.beta_min, self.beta_max, self.T).to(self.device)\n",
    "            self.alpha = (1 - self.beta).to(self.device)\n",
    "            self.alpha_bar = self.alpha.cumprod(dim=0).to(self.device)\n",
    "\n",
    "        elif self.schedule == 'cosine':\n",
    "            steps = torch.linspace(0, torch.pi, self.T).to(self.device)\n",
    "            self.beta = ((torch.cos(steps) + 1) * 0.5 * (self.beta_max - self.beta_min) + self.beta_min).flip(0)\n",
    "            self.alpha = (1 - self.beta).to(self.device)\n",
    "            self.alpha_bar = self.alpha.cumprod(dim=0).to(self.device)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown schedule type: {self.schedule}\")\n",
    "\n",
    "    def forward_diffusion(self, x0, t):\n",
    "        \"\"\"\n",
    "        Perform the forward diffusion process.\n",
    "        Args:\n",
    "            x0: Original data (e.g., images).\n",
    "            t: Timesteps (tensor of integers).\n",
    "        Returns:\n",
    "            xt: Noisy data at timestep t.\n",
    "            epsilon: The noise added.\n",
    "        \"\"\"\n",
    "        eps = torch.randn_like(x0)\n",
    "\n",
    "        sqrt_alpha_bar_t = self.alpha_bar[t].sqrt().view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar_t = (1 - self.alpha_bar[t]).sqrt().view(-1, 1, 1, 1)\n",
    "\n",
    "        xt = sqrt_alpha_bar_t * x0 + sqrt_one_minus_alpha_bar_t * eps\n",
    "\n",
    "        return xt, eps\n",
    "\n",
    "    def reverse_diffusion(self, xt, t, model, time_embedding):\n",
    "        \"\"\"\n",
    "        Perform the reverse diffusion process.\n",
    "        Args:\n",
    "            xt: Noisy data at timestep t.\n",
    "            t: Timesteps for batch (tensor of integers).\n",
    "            model: Model used for reverse diffusion.\n",
    "            time_embedding: Time embedding object.\n",
    "        Returns:\n",
    "            xr: Reconstructed data at timestep t.\n",
    "        \"\"\"\n",
    "        z = torch.where((t > 1).view(-1, 1, 1, 1), torch.randn_like(xt), torch.zeros_like(xt))\n",
    "\n",
    "        # extract time embedding\n",
    "        time_emb = time_embedding(t)\n",
    "\n",
    "        # predict noise\n",
    "        eps_theta = model(xt, time_emb)\n",
    "\n",
    "        sqrt_alpha_t = self.alpha[t].sqrt().view(-1, 1, 1, 1)  # we need to reshape the tensor to match the shape of xt (same goes for the other tensors)\n",
    "        beta_t = self.beta[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar_t = (1 - self.alpha_bar[t]).sqrt().view(-1, 1, 1, 1)\n",
    "        sigma_t = beta_t.sqrt()\n",
    "\n",
    "        xt_minus_one = 1. / sqrt_alpha_t * (xt - (beta_t / sqrt_one_minus_alpha_bar_t) * eps_theta) + sigma_t * z\n",
    "\n",
    "        return xt_minus_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "cHIB16dg7qaH"
   },
   "outputs": [],
   "source": [
    "### Function for training DDPM ###\n",
    "def train_ddpm_epoch(model: object, diffusion: object, time_embedding: object, train_loader: object, epoch: int, device: str, optimizer: object, lr_scheduler: object):\n",
    "    \"\"\"\n",
    "    This function implements Algorithm 1 from the paper, specifically it trains the U-Net model for one epoch.\n",
    "    Args:\n",
    "        model: U-Net model.\n",
    "        diffusion: Diffusion object.\n",
    "        time_embedding: Time embedding object.\n",
    "        train_loader: DataLoader for training data.\n",
    "        epoch: Current epoch.\n",
    "        device: Device to use (e.g., 'cuda' or 'cpu').\n",
    "        optimizer: Optimizer for training.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Training epoch {epoch}...\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, (x0, _) in enumerate(train_loader):\n",
    "        x0 = x0.to(device)\n",
    "\n",
    "        # Sample random timestep\n",
    "        t = torch.randint(0, diffusion.T, (x0.size(0),), device=device)\n",
    "\n",
    "        # Get sinusoidal time embedding\n",
    "        time_emb = time_embedding(t)\n",
    "\n",
    "        # Forward diffusion\n",
    "        xt, epsilon = diffusion.forward_diffusion(x0, t)\n",
    "\n",
    "        # Predict noise\n",
    "        epsilon_pred = model(xt, time_emb)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(epsilon_pred, epsilon)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Use learning rate scheduler\n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        # Log loss\n",
    "        wandb.log({\"loss\": loss.item(), \"epoch\": epoch, \"lr\": optimizer.param_groups[0]['lr']})\n",
    "\n",
    "        if epoch > 1:\n",
    "            wandb.log({\"loss after 1st epoch\": loss.item()})\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, batch {i}: loss={loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gRuR5smU7qaH"
   },
   "outputs": [],
   "source": [
    "### Function to sample from DDPM ###\n",
    "\n",
    "def sample_ddpm(model: object, diffusion: object, time_embedding: object, device: str, num_samples: int = 16, dataset: str = 'MNIST'):\n",
    "    \"\"\"\n",
    "    This function implements Algorithm 2 from the paper, specifically it samples from the U-Net model.\n",
    "    Args:\n",
    "        model: U-Net model.\n",
    "        diffusion: Diffusion object.\n",
    "        time_embedding: Time embedding object.\n",
    "        device: Device to use (e.g., 'cuda' or 'cpu').\n",
    "        num_samples: Number of samples to generate.\n",
    "        dataset: Dataset to use ('MNIST' or 'CIFAR10').\n",
    "    Returns:\n",
    "        samples: Generated samples.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Sampling {num_samples} samples...\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1. Initialize samples from standard gaussian distribution\n",
    "        if dataset == 'MNIST':\n",
    "            x = torch.randn((num_samples, 1, 28, 28), device=device)\n",
    "        elif dataset == 'CIFAR10':\n",
    "            x = torch.randn((num_samples, 3, 32, 32), device=device)\n",
    "\n",
    "        # 2. iterative reverse diffusion\n",
    "        for t in reversed(range(1, diffusion.T + 1)):\n",
    "            # prepare timestep tensor\n",
    "            t_tensor = torch.full((num_samples,), t - 1, device=device, dtype=torch.long)\n",
    "\n",
    "            # Perform reverse diffusion step\n",
    "            x = diffusion.reverse_diffusion(x, t_tensor, model, time_embedding)\n",
    "\n",
    "        # 3. Return samples\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LBVhXHg7qaH"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TW5PxxjO7qaH"
   },
   "outputs": [],
   "source": [
    "### Dataloader ###\n",
    "def get_dataloader(dataset = \"MNIST\", batch_size = 128):\n",
    "    transforms = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                # No flips in MNIST as it disturbes the interpretation of numbers?\n",
    "                #torchvision.transforms.Resize((32, 32), interpolation=torchvision.transforms.InterpolationMode.BICUBIC), # TODO: Resize to 32 x 32 for unet implementation?\n",
    "                torchvision.transforms.Normalize((0.5,), (0.5,))  # Correpsonds to scaling between [-1, 1] --> (x - 0.5)/0.5\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    transforms_cifar10_train = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.RandomHorizontalFlip(),\n",
    "                #torchvision.transforms.Resize((32, 32), interpolation=torchvision.transforms.InterpolationMode.BICUBIC), # TODO: Match paper or resize to 28 x 28 for unet implementation\n",
    "                torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Scaling between [-1, 1]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    transforms_cifar10_test = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                #torchvision.transforms.Resize((32, 32), interpolation=torchvision.transforms.InterpolationMode.BICUBIC), # TODO: Match paper or resize to 28 x 28 for unet implementation\n",
    "                torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Scaling between [-1, 1]\n",
    "            ]\n",
    "        )\n",
    "    if dataset == \"MNIST\":\n",
    "        trainset = MNIST(\"./temp/\", train=True, download=True, transform= transforms)\n",
    "        testset = MNIST(\"./temp/\", train=False, download=True, transform= transforms)\n",
    "    elif dataset == \"CIFAR10\":\n",
    "        trainset = CIFAR10(\"./temp/\", train=True, download=True, transform= transforms_cifar10_train)\n",
    "        testset = CIFAR10(\"./temp/\", train=False, download=True, transform= transforms_cifar10_test)\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(trainset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "    test_dataloader = DataLoader(testset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "def get_dataloader_evaluation(dataset = \"MNIST\", batch_size = 128):\n",
    "    transforms_mnist = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "            torchvision.transforms.Resize((299, 299)),  # Resize for InceptionV3\n",
    "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  #normalize for InceptionV3\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    transforms_cifar10 = torchvision.transforms.Compose(\n",
    "          [\n",
    "              torchvision.transforms.Resize((299, 299)),  # Resize for InceptionV3\n",
    "              torchvision.transforms.ToTensor(),\n",
    "              torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # CIFAR normalization\n",
    "          ])\n",
    "\n",
    "\n",
    "    if dataset == \"MNIST\":\n",
    "        trainset = MNIST(\"./temp/\", train=True, download=True, transform= transforms_mnist)\n",
    "    elif dataset == \"CIFAR10\":\n",
    "        trainset = CIFAR10(\"./temp/\", train=True, download=True, transform= transforms_cifar10)\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(trainset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI3vWLpX7qaH"
   },
   "source": [
    "### Custom learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MbNfhxeq7qaI"
   },
   "outputs": [],
   "source": [
    "### Custom Learning Rate Scheduler ###\n",
    "class WarmUpPiecewiseConstantSchedule(_LRScheduler):\n",
    "    '''\n",
    "    This class implements a learning rate scheduler that combines a warm-up phase with a piecewise constant decay.\n",
    "    That is, the learning rate is increased linearly from 0 to the base learning rate during the warm-up phase, and then\n",
    "    decreased by a factor of decay_ratio at each epoch in decay_epochs (similarly to MultiStepLR).\n",
    "    Note: can maybe be done better, but needed it done quickly...\n",
    "\n",
    "    Note: I implemented this class as part of my bachelor's thesis (jonathansim).\n",
    "    '''\n",
    "    def __init__(self, optimizer, steps_per_epoch, base_lr, lr_decay_ratio, lr_decay_epochs, warmup_epochs, last_epoch=-1):\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.base_lr = base_lr\n",
    "        self.decay_ratio = lr_decay_ratio\n",
    "        self.decay_epochs = lr_decay_epochs\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.decay_steps = [e * steps_per_epoch for e in lr_decay_epochs]  # Convert epochs to steps\n",
    "        super(WarmUpPiecewiseConstantSchedule, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        # Calculate the current step\n",
    "        lr_step = self.last_epoch\n",
    "        lr_epoch = lr_step / self.steps_per_epoch\n",
    "        learning_rate = self.base_lr\n",
    "\n",
    "        # Warm-Up Phase\n",
    "        if lr_epoch < self.warmup_epochs:\n",
    "            learning_rate = self.base_lr * lr_step / (self.warmup_epochs * self.steps_per_epoch)\n",
    "        else:\n",
    "            # Piecewise Constant Decay Phase\n",
    "            for i, start_step in enumerate(self.decay_steps):\n",
    "                if lr_step >= start_step:\n",
    "                    learning_rate = self.base_lr * (self.decay_ratio ** (i + 1))\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        return [learning_rate for _ in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1MUWK8hb7qaI"
   },
   "outputs": [],
   "source": [
    "### Function for setting seed ###\n",
    "def set_training_seed(seed):\n",
    "    # Function to set the different seeds\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSTGL-Rp7qaI"
   },
   "source": [
    "### Evaluation functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "KsrIprS57qaI"
   },
   "outputs": [],
   "source": [
    "### Frechet Inception Distance ###\n",
    "def calculate_fid(feat1, feat2):\n",
    "    '''\n",
    "    calculate fid between two sets of images\n",
    "    feat1: real images - feature vector obtained from inception model (or alternative model)\n",
    "    feat2: generated images - feature vector obtained from inception model (or alternative model)\n",
    "    return: fid score\n",
    "    '''\n",
    "    # calculate mean and covariance of features\n",
    "    mu1, sigma1 = np.mean(feat1, axis=0), np.cov(feat1, rowvar=False)\n",
    "    mu2, sigma2 = np.mean(feat2, axis=0), np.cov(feat2, rowvar=False)\n",
    "    print(\"Mean1: \", np.shape(mu1))\n",
    "    print(\"Mean2: \", np.shape(mu2))\n",
    "    # sum squared difference between means\n",
    "    diff = np.sum((mu1 - mu2)**2)\n",
    "    print(\"diff: \", diff)\n",
    "    # calculate \"geometric mean\" of covariance matrices\n",
    "    covmean = fractional_matrix_power(sigma1.dot(sigma2), 0.5)\n",
    "    # check for imaginary numbers\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = np.real(covmean)\n",
    "\n",
    "    fid = diff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "\n",
    "def compute_activations(samples, model, batched = False, batch_size = 100):\n",
    "    \"\"\"\n",
    "    Compute activations from samples based on model.\n",
    "    Args:\n",
    "        samples: Tensor of shape (num_samples, channels, height, width)\n",
    "        model: Pre-trained model to extract features\n",
    "    Returns:\n",
    "        activations: Feature activations for input images\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    samples = samples.to(next(model.parameters()).device)  # Move to the same device as the model\n",
    "\n",
    "    if batched:\n",
    "        activations = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, samples.size(0), batch_size):\n",
    "                batch = samples[i:i+batch_size]\n",
    "                batch_activations = model(batch).cpu().numpy()\n",
    "                activations.append(batch_activations)\n",
    "        return np.concatenate(activations, axis=0)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            activations = model(samples)\n",
    "        return activations.cpu().numpy()\n",
    "\n",
    "\n",
    "def get_images_in_batches(dataloader, num_images=None):\n",
    "    \"\"\"\n",
    "    Fetch e.g. CIFAR-10 images in smaller batches without concatenating them.\n",
    "    Args:\n",
    "        dataloader: PyTorch DataLoader providing the dataset.\n",
    "        num_images: Number of images to fetch.\n",
    "    Yields:\n",
    "        Batches of real images from the dataset.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for images, _ in dataloader:\n",
    "        count += len(images)\n",
    "        if num_images and count >= num_images:\n",
    "            yield images\n",
    "            break\n",
    "\n",
    "        # Yield images batch-by-batch\n",
    "        yield images\n",
    "\n",
    "        # Free memory\n",
    "        del images\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def full_fid(generated_samples, data, num_images = 10000):\n",
    "    \"\"\"\n",
    "    Computes the Fréchet Inception Distance (FID) between generated images and real images.\n",
    "    Args:\n",
    "        generated_samples (torch.Tensor): Tensor of generated samples\n",
    "            with shape (num_samples, channels, height, width).\n",
    "        data (str): MNIST or CIFAR10\n",
    "        num_images (int, optional): Number of real images to use for FID computation.\n",
    "            Defaults to 10,000.\n",
    "    Returns:\n",
    "        float: FID score between the generated and real images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load pre-trained InceptionV3\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                    \"mps\" if torch.backends.mps.is_available() else\n",
    "                    \"cpu\")\n",
    "    \n",
    "    # PYTORCH_ENABLE_MPS_FALLBACK=1 # set environment to enable to fallback to cpu\n",
    "    inception_classifier = models.inception_v3(weights='DEFAULT', transform_input=False)\n",
    "    inception_classifier.fc = torch.nn.Identity()  # remove classification layer to extract features\n",
    "    inception_classifier = inception_classifier.to(device)\n",
    "\n",
    "    # Get train samples\n",
    "    dataloader = get_dataloader_evaluation(dataset = data, batch_size = 100)\n",
    "    print(\"DATA: \", data)\n",
    "    # Get activations for real images\n",
    "    real_image_activations = []\n",
    "    for batch in get_images_in_batches(dataloader, num_images=num_images):\n",
    "        activation = compute_activations(batch, inception_classifier)\n",
    "        real_image_activations.extend(activation)\n",
    "    real_image_activations = real_image_activations[:num_images]\n",
    "\n",
    "    # Get activations of generated images\n",
    "    min_val = generated_samples.min()\n",
    "    max_val = generated_samples.max()\n",
    "    resized_samples = (generated_samples - min_val) / (max_val - min_val) # rescale pixel values to [0,1]\n",
    "    if data == \"MNIST\":\n",
    "        resized_samples = resized_samples.repeat(1, 3, 1, 1)\n",
    "    resized_samples = torchvision.transforms.functional.resize(resized_samples, size = (299, 299)) # resize for inception\n",
    "    resized_samples = torchvision.transforms.functional.normalize(resized_samples,\n",
    "        mean=torch.tensor([0.485, 0.456, 0.406]).view(1, -1, 1, 1).to(device),\n",
    "        std=torch.tensor([0.229, 0.224, 0.225]).view(1, -1, 1, 1).to(device)\n",
    "    )\n",
    "    generated_image_activations = compute_activations(resized_samples, inception_classifier, batched = True)\n",
    "    print(np.shape(generated_image_activations))\n",
    "    print(np.shape(real_image_activations))\n",
    "    fid = calculate_fid(real_image_activations, generated_image_activations)\n",
    "\n",
    "    return fid\n",
    "\n",
    "# full_fid splitted up\n",
    "def get_inception_model():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                    \"mps\" if torch.backends.mps.is_available() else\n",
    "                    \"cpu\")\n",
    "    inception_classifier = models.inception_v3(weights='DEFAULT', transform_input=False)\n",
    "    inception_classifier.fc = torch.nn.Identity()  # remove classification layer to extract features\n",
    "    inception_classifier = inception_classifier.to(device)\n",
    "    return inception_classifier\n",
    "\n",
    "\n",
    "def get_real_image_activations(inception_classifier, data, num_images = 10000):\n",
    "    # Get train samples\n",
    "    dataloader = get_dataloader_evaluation(dataset = data, batch_size = 100)\n",
    "    print(\"DATA: \", data)\n",
    "    # Get activations for real images\n",
    "    real_image_activations = []\n",
    "    for batch in get_images_in_batches(dataloader, num_images=num_images):\n",
    "        activation = compute_activations(batch, inception_classifier)\n",
    "        real_image_activations.extend(activation)\n",
    "    real_image_activations = real_image_activations[:num_images]\n",
    "    return real_image_activations\n",
    "\n",
    "\n",
    "def calculate_fid_generated_samples(generated_samples, real_image_activations, inception_classifier, device, data, num_images = 10000):\n",
    "    # Get activations of generated images\n",
    "    min_val = generated_samples.min()\n",
    "    max_val = generated_samples.max()\n",
    "    resized_samples = (generated_samples - min_val) / (max_val - min_val) # rescale pixel values to [0,1]\n",
    "    if data == \"MNIST\":\n",
    "        resized_samples = resized_samples.repeat(1, 3, 1, 1)\n",
    "    resized_samples = torchvision.transforms.functional.resize(resized_samples, size = (299, 299)) # resize for inception\n",
    "    resized_samples = torchvision.transforms.functional.normalize(resized_samples,\n",
    "        mean=torch.tensor([0.485, 0.456, 0.406]).view(1, -1, 1, 1).to(device),\n",
    "        std=torch.tensor([0.229, 0.224, 0.225]).view(1, -1, 1, 1).to(device)\n",
    "    )\n",
    "    generated_image_activations = compute_activations(resized_samples, inception_classifier, batched = True)\n",
    "    print(np.shape(generated_image_activations))\n",
    "    print(np.shape(real_image_activations))\n",
    "    fid = calculate_fid(real_image_activations, generated_image_activations)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "bgv1eLCA7qaI"
   },
   "outputs": [],
   "source": [
    "### Inception Score ###\n",
    "def calculate_inception_score(samples, dataset, device, batch_size=32, splits=10):\n",
    "    \"\"\"\n",
    "    Calculates the Inception Score for generated samples.\n",
    "    Inspired by: https://github.com/sbarratt/inception-score-pytorch/blob/master/inception_score.py\n",
    "    and paper: https://papers.nips.cc/paper_files/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf\n",
    "\n",
    "    Args:\n",
    "        samples (torch.Tensor): Generated samples with shape (num_samples, channels, height, width).\n",
    "        device (torch.device): Device to perform calculations on\n",
    "        dataset (str): One of the two datasets\n",
    "        batch_size (int): Batch size for processing samples.\n",
    "        splits (int): Number of splits for calculating the mean and standard deviation of the score.\n",
    "\n",
    "    Returns:\n",
    "        float, float: Mean and standard deviation of the Inception Score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load pretrained InceptionV3 model\n",
    "    inception_model = models.inception_v3(pretrained=True)  # keep the final classification layer\n",
    "    inception_model = inception_model.to(device)\n",
    "\n",
    "    inception_model.eval()\n",
    "    samples = samples.to(device)\n",
    "    num_samples = samples.size(0)\n",
    "\n",
    "    # Placeholder for storing predictions\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch = samples[i:i + batch_size]\n",
    "            if dataset == \"MNIST\":\n",
    "                batch = batch.repeat(1, 3, 1, 1)  # repeat channels for Inception when using MNIST\n",
    "            batch = transforms.functional.resize(batch, size=(299, 299))  # resize for Inception\n",
    "            batch = transforms.functional.normalize(batch,\n",
    "                mean=torch.tensor([0.485, 0.456, 0.406]).to(device),\n",
    "                std=torch.tensor([0.229, 0.224, 0.225]).to(device),\n",
    "            )\n",
    "            pred = inception_model(batch)\n",
    "            pred = F.softmax(pred, dim=1)\n",
    "            preds.append(pred.cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "    # Compute IS\n",
    "    scores = []\n",
    "    split_size = preds.shape[0] // splits\n",
    "    for i in range(splits):\n",
    "        part = preds[i * split_size: (i + 1) * split_size] # probability of labels conditioned on image\n",
    "        p_y = np.mean(part, axis=0)\n",
    "        kl_div = part * (np.log(part) - np.log(p_y[None, :]))\n",
    "        kl_div = np.sum(kl_div, axis=1)\n",
    "        scores.append(np.exp(np.mean(kl_div)))\n",
    "\n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrSW_g5r7qaI"
   },
   "source": [
    "## Training of Models ##\n",
    "Since we run various experiments with two different datasets MNIST and CIFAR10 and different parameters, we made a general pipeline for training and evaluating our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "o9A_488D7qaI"
   },
   "outputs": [],
   "source": [
    "### Main training script ###\n",
    "\n",
    "## Argument parser\n",
    "parser = argparse.ArgumentParser(description='Train (and sample from) DDPM framework.')\n",
    "\n",
    "## Add arguments\n",
    "parser.add_argument('--T', type=int, default=1000, help='Total timesteps.')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='Batch size.')\n",
    "parser.add_argument('--num_epochs', type=int, default=5, help='Number of epochs.')\n",
    "parser.add_argument('--lr', type=float, default=2e-4, help='Learning rate.')\n",
    "parser.add_argument('--dataset', type=str, default='MNIST', help='Dataset to use (MNIST or CIFAR10).')\n",
    "parser.add_argument('--save_model', type=bool, default=True, help='Save model after training.')\n",
    "parser.add_argument('--wandb', default=\"online\", type=str, choices=[\"online\", \"disabled\"] , help=\"whether to track with weights and biases or not\")\n",
    "parser.add_argument('--heads', type=int, default=4, help='Number of heads for attention mechanism.')\n",
    "parser.add_argument('--noise_scheduler', type=str, default='cosine', choices=[\"linear\", \"cosine\"], help='Noise scheduler type.')\n",
    "parser.add_argument('--lr_scheduler', type=str, default='none', choices=[\"none\", \"warmup_linear\"], help='Learning rate scheduler type.')\n",
    "parser.add_argument('--seed', default=1, type=int, help=\"seed for reproducibility\")\n",
    "parser.add_argument('--fid', default=True, type=bool, help=\"Calculate FID for 10000 images after training\")\n",
    "parser.add_argument('--calculate_fid_25', default=True, type=bool, help=\"Calculate FID for 2500 images after every fid_epoch_modulo (e.g. 25th) epoch\")\n",
    "parser.add_argument('--fid_epoch_modulo', default=50, type=int, help=\"Which epochs to run fid\")\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Parse arguments\n",
    "    #args = parser.parse_args()\n",
    "    args = args # Redefine argument parsing when running in notebook\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    set_training_seed(args.seed)\n",
    "\n",
    "    # Unpack arguments\n",
    "    T = args.T\n",
    "    batch_size = args.batch_size\n",
    "    num_epochs = args.num_epochs\n",
    "    lr = args.lr\n",
    "    dataset = args.dataset\n",
    "    save_model = args.save_model\n",
    "    heads = args.heads\n",
    "    noise_scheduler = args.noise_scheduler\n",
    "    lr_scheduler = args.lr_scheduler\n",
    "    calculate_fid = args.fid\n",
    "    calculate_fid_25 = args.calculate_fid_25\n",
    "    fid_epoch_modulo = args.fid_epoch_modulo\n",
    "\n",
    "\n",
    "    # Scheduler parameters\n",
    "    warm_up_epochs = 2\n",
    "    if dataset == 'MNIST':\n",
    "        lr_decay_epochs = [20, 40, 60]\n",
    "    elif dataset == 'CIFAR10':\n",
    "        lr_decay_epochs = [200, 400, 500]\n",
    "\n",
    "\n",
    "    save_dir = \"./saved_models\"  # Directory to save the trained model\n",
    "\n",
    "    # Set number of input channels\n",
    "    num_input_channels = 1 if dataset == 'MNIST' else 3\n",
    "\n",
    "    # Set mode for Weights and Biases\n",
    "    mode_for_wandb = args.wandb\n",
    "    run_name = f\"{dataset}_bs_{batch_size}_Nscheduler_{noise_scheduler}_heads_{heads}_LRs_{lr_scheduler}_seed_{args.seed}\"\n",
    "\n",
    "    # Initialize Weights and Biases\n",
    "    wandb.init(project='ddpm', entity='dl_ddpm', mode=mode_for_wandb, name=run_name)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cpu\")\n",
    "    print(f\"Using Device: {device}\")\n",
    "\n",
    "    # Dataloader\n",
    "    train_loader, _ = get_dataloader(dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize components\n",
    "    diffusion = Diffusion(T=T, beta_min=10e-5, beta_max=0.02, schedule=noise_scheduler, device=device)\n",
    "\n",
    "    time_embedding = SinusoidalPositionEmbeddings(total_time_steps=T, time_emb_dims=128, time_emb_dims_exp=512).to(device)\n",
    "\n",
    "    model = UNet(input_channels=num_input_channels,\n",
    "                 resolutions=[64, 128, 256, 512],\n",
    "                 time_emb_dims=512,\n",
    "                 dropout=0.1,\n",
    "                 use_attention=[False, True, False],\n",
    "                 heads=heads).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    if lr_scheduler == \"warmup_linear\":\n",
    "        scheduler = WarmUpPiecewiseConstantSchedule(optimizer=optimizer, steps_per_epoch=len(train_loader), base_lr=args.lr,\n",
    "                                                    lr_decay_ratio=0.2, lr_decay_epochs=lr_decay_epochs, warmup_epochs=warm_up_epochs)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    if calculate_fid_25:\n",
    "        inception_model = get_inception_model()\n",
    "        real_activations = get_real_image_activations(inception_classifier=inception_model, data=dataset, num_images=2500)\n",
    "    fid_epochs = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_ddpm_epoch(model, diffusion, time_embedding, train_loader, epoch, device, optimizer, scheduler)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            # Generate samples\n",
    "            samples = sample_ddpm(model, diffusion, time_embedding, device, num_samples=2, dataset=dataset)\n",
    "            wandb.log({\"Generated Samples\": [wandb.Image(sample, caption=f\"Epoch {epoch}\") for sample in samples]})\n",
    "\n",
    "        if calculate_fid_25:\n",
    "            if epoch == 1:\n",
    "                samples = sample_ddpm(model, diffusion, time_embedding, device, num_samples=2500, dataset=dataset)\n",
    "                fid_epochs.append(calculate_fid_generated_samples(generated_samples=samples,\n",
    "                                                                  real_image_activations=real_activations,\n",
    "                                                                  inception_classifier=inception_model,\n",
    "                                                                  data = dataset,\n",
    "                                                                  num_images = 2500,\n",
    "                                                                  device = device))\n",
    "                wandb.log({\"FID\": fid_epochs[-1]})\n",
    "            if epoch % fid_epoch_modulo == 0:\n",
    "                samples = sample_ddpm(model, diffusion, time_embedding, device, num_samples=2500, dataset=dataset)\n",
    "                fid_epochs.append(calculate_fid_generated_samples(generated_samples=samples,\n",
    "                                                                  real_image_activations=real_activations,\n",
    "                                                                  inception_classifier=inception_model,\n",
    "                                                                  data = dataset,\n",
    "                                                                  num_images = 2500,\n",
    "                                                                  device = device))\n",
    "                wandb.log({\"FID\": fid_epochs[-1]})\n",
    "    print(\"FID_epochs: \", fid_epochs)\n",
    "    if save_model:\n",
    "        final_save_path = f\"{save_dir}/ddpm_{dataset}_{noise_scheduler}_heads_{heads}_LRs_{lr_scheduler}_seed{args.seed}.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            \"embedding_state_dict\": time_embedding.state_dict(),\n",
    "        }, final_save_path)\n",
    "\n",
    "        print(f\"Model and embedding saved at: {final_save_path}\")\n",
    "\n",
    "    if calculate_fid:\n",
    "        # Calculate FID\n",
    "        samples = sample_ddpm(model, diffusion, time_embedding, device, num_samples=10000, dataset=dataset)\n",
    "        print(\"Samples generated successfully!\")\n",
    "        print(\"Shape samples: \", np.shape(samples))\n",
    "\n",
    "        fid = full_fid(samples, data = dataset, num_images = 10000)\n",
    "        wandb.log({\"full_FID\": fid})\n",
    "        print(\"Fid: \", fid)\n",
    "        # Finish Weights and Biases run\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMseNIUI7qaI"
   },
   "source": [
    "## Examples using the script\n",
    "Since it is not feasible to run all our experiments in a notebook we provide a small example of how our setup runs. In reality we ran the main script with different arguments corresponding to our specific experiments.\n",
    "1) An example of MNIST - we only train for 5 epochs in our real experiments we run 75\n",
    "2) An example of CIFAR10 - we only train for 5 epochs in the notebook, but in our real experiments we run 600 epochs\n",
    "3) An example of FID and IS calculation based on a saved model - In our experiments we sample 10K generated images, but here we only sample 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zFf9_axv7qaI",
    "outputId": "272831a3-e31e-445f-f706-99979740806e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./temp/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./temp/MNIST/raw/train-images-idx3-ubyte.gz to ./temp/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./temp/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 347kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./temp/MNIST/raw/train-labels-idx1-ubyte.gz to ./temp/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./temp/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.77MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./temp/MNIST/raw/t10k-images-idx3-ubyte.gz to ./temp/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./temp/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.91MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./temp/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./temp/MNIST/raw\n",
      "\n",
      "Training epoch 1...\n",
      "Epoch 1, batch 0: loss=1.0248407125473022\n",
      "Epoch 1, batch 100: loss=0.09949691593647003\n",
      "Epoch 1, batch 200: loss=0.06893058121204376\n",
      "Epoch 1, batch 300: loss=0.04778468608856201\n",
      "Epoch 1, batch 400: loss=0.05255862697958946\n",
      "Training epoch 2...\n",
      "Epoch 2, batch 0: loss=0.05860847234725952\n",
      "Epoch 2, batch 100: loss=0.03983432799577713\n",
      "Epoch 2, batch 200: loss=0.035100847482681274\n",
      "Epoch 2, batch 300: loss=0.04526690021157265\n",
      "Epoch 2, batch 400: loss=0.03606978803873062\n",
      "Training epoch 3...\n",
      "Epoch 3, batch 0: loss=0.03434152528643608\n",
      "Epoch 3, batch 100: loss=0.032479044049978256\n",
      "Epoch 3, batch 200: loss=0.04145487770438194\n",
      "Epoch 3, batch 300: loss=0.03720972314476967\n",
      "Epoch 3, batch 400: loss=0.029410317540168762\n",
      "Training epoch 4...\n",
      "Epoch 4, batch 0: loss=0.0346800722181797\n",
      "Epoch 4, batch 100: loss=0.032416850328445435\n",
      "Epoch 4, batch 200: loss=0.03310177102684975\n",
      "Epoch 4, batch 300: loss=0.03253283351659775\n",
      "Epoch 4, batch 400: loss=0.033776480704545975\n",
      "Training epoch 5...\n",
      "Epoch 5, batch 0: loss=0.0302775539457798\n",
      "Epoch 5, batch 100: loss=0.030553089454770088\n",
      "Epoch 5, batch 200: loss=0.03257891163229942\n",
      "Epoch 5, batch 300: loss=0.029009146615862846\n",
      "Epoch 5, batch 400: loss=0.02578519657254219\n",
      "Sampling 2 samples...\n",
      "FID_epochs:  []\n"
     ]
    }
   ],
   "source": [
    "### MNIST Example ###\n",
    "from types import SimpleNamespace\n",
    "import sys\n",
    "args = SimpleNamespace(\n",
    "    T=1000,\n",
    "    batch_size=128,\n",
    "    num_epochs=5, #normally 75\n",
    "    lr=2e-4,\n",
    "    dataset=\"MNIST\",\n",
    "    save_model=False,\n",
    "    wandb=\"disabled\",\n",
    "    heads=1, # This is changed for the different models\n",
    "    noise_scheduler=\"linear\", # This is changed to \"cosine\" for models using a cosine noise schedule\n",
    "    lr_scheduler=\"none\", # This is changed to \"warmup_linear\" when using the custom lr_scheduler\n",
    "    seed=1,\n",
    "    fid=False,\n",
    "    calculate_fid_25=False,\n",
    "    fid_epoch_modulo=50\n",
    ")\n",
    "\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "ChQpiFW57qaI",
    "outputId": "005d5685-1848-460a-d543-91a706983a33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./temp/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:05<00:00, 30.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./temp/cifar-10-python.tar.gz to ./temp/\n",
      "Files already downloaded and verified\n",
      "Training epoch 1...\n",
      "Epoch 1, batch 0: loss=1.0528299808502197\n",
      "Epoch 1, batch 100: loss=0.15049698948860168\n",
      "Epoch 1, batch 200: loss=0.10594990104436874\n",
      "Epoch 1, batch 300: loss=0.08235377073287964\n",
      "Training epoch 2...\n",
      "Epoch 2, batch 0: loss=0.07530054450035095\n",
      "Epoch 2, batch 100: loss=0.08824054896831512\n",
      "Epoch 2, batch 200: loss=0.07016375660896301\n",
      "Epoch 2, batch 300: loss=0.06308020651340485\n",
      "Training epoch 3...\n",
      "Epoch 3, batch 0: loss=0.06016600877046585\n",
      "Epoch 3, batch 100: loss=0.04957980662584305\n",
      "Epoch 3, batch 200: loss=0.06536881625652313\n",
      "Epoch 3, batch 300: loss=0.05608316510915756\n",
      "Training epoch 4...\n",
      "Epoch 4, batch 0: loss=0.05260009691119194\n",
      "Epoch 4, batch 100: loss=0.05747954174876213\n",
      "Epoch 4, batch 200: loss=0.05371551960706711\n",
      "Epoch 4, batch 300: loss=0.06030559539794922\n",
      "Training epoch 5...\n",
      "Epoch 5, batch 0: loss=0.0474037230014801\n",
      "Epoch 5, batch 100: loss=0.0632072240114212\n",
      "Epoch 5, batch 200: loss=0.06077512726187706\n",
      "Epoch 5, batch 300: loss=0.03385879844427109\n",
      "Sampling 2 samples...\n",
      "FID_epochs:  []\n"
     ]
    }
   ],
   "source": [
    "### CIFAR10 Example ###\n",
    "args = SimpleNamespace(\n",
    "    T=1000,\n",
    "    batch_size=128,\n",
    "    num_epochs=5, # Normally 600\n",
    "    lr=2e-4,\n",
    "    dataset=\"CIFAR10\",\n",
    "    save_model=False,\n",
    "    wandb=\"disabled\",\n",
    "    heads=1,\n",
    "    noise_scheduler=\"linear\",\n",
    "    lr_scheduler=\"none\",\n",
    "    seed=1,\n",
    "    fid=False,\n",
    "    calculate_fid_25=False,\n",
    "    fid_epoch_modulo=50\n",
    ")\n",
    "\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "_Joh4RI07qaI",
    "outputId": "cd858fdf-9d79-40ee-9f2f-c875efd96457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n",
      "Model_path:  trained_models/ddpm_CIFAR10_cosine_heads_4_LRs_warmup_linear_seed7.pth\n",
      "Sampling 50 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sb/klhtpy1n7jd649__g8r7xj300000gn/T/ipykernel_62026/4234486406.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples generated successfully!\n",
      "Files already downloaded and verified\n",
      "DATA:  CIFAR10\n",
      "(50, 2048)\n",
      "(50, 2048)\n",
      "Mean1:  (2048,)\n",
      "Mean2:  (2048,)\n",
      "diff:  15.942528\n",
      "Fid:  221.8422500873504\n",
      "Inception Score: 2.5218093395233154\n"
     ]
    }
   ],
   "source": [
    "### Example of FID + IS for trained CIFAR model ###\n",
    "## N.B. \n",
    "# When trying to compute the FID on GPU, this consistently crashes and we have unfortunately not been able to solve this issue. \n",
    "# We hope showcasing the code with an example where the FID is calculated on CPU (with much fewer samples) will be sufficient.\n",
    "\n",
    "set_training_seed(7)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                    #\"mps\" if torch.backends.mps.is_available() else\n",
    "                    # \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using Device: {device}\")\n",
    "\n",
    "# Step 1: Initialize the Sinusoidal Embeddings\n",
    "time_embedding = SinusoidalPositionEmbeddings(total_time_steps=1000, time_emb_dims=128, time_emb_dims_exp=512).to(device)\n",
    "\n",
    "# Step 2: Initialize the U-Net and Diffusion\n",
    "unet = UNet(input_channels=3, resolutions=[64, 128, 256, 512], time_emb_dims=512, dropout=0.1, use_attention=[False, True, False], heads=4).to(device)\n",
    "diffusion = Diffusion(T=1000, beta_min=10e-5, beta_max=0.02, schedule='cosine', device=device)\n",
    "\n",
    "# Step 3: Load the trained model\n",
    "model_path = \"trained_models/ddpm_CIFAR10_cosine_heads_4_LRs_warmup_linear_seed7.pth\" # insert model path for model we wish to evaluate\n",
    "print(\"Model_path: \", model_path)\n",
    "saved = torch.load(model_path, map_location=device)\n",
    "\n",
    "unet.load_state_dict(saved[\"model_state_dict\"])\n",
    "time_embedding.load_state_dict(saved[\"embedding_state_dict\"])\n",
    "\n",
    "\n",
    "# Step 4: Generate samples\n",
    "samples = sample_ddpm(unet, diffusion, time_embedding, device, num_samples=50, dataset='CIFAR10') # usually 10K images, but here lowered for faster calculation\n",
    "print(\"Samples generated successfully!\")\n",
    "\n",
    "PYTORCH_ENABLE_MPS_FALLBACK=1 \n",
    "fid = full_fid(samples, data = \"CIFAR10\", num_images = 50) # usually 10K images, but here lowered for faster calculation\n",
    "print(\"Fid: \", fid)\n",
    "\n",
    "# Calculate Inception Score\n",
    "mean_is, std_is = calculate_inception_score(samples, \"CIFAR10\", device)\n",
    "print(f\"Inception Score: {mean_is}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-c_Eya3-7qaJ"
   },
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "4Ie5Mp8Q7qaJ"
   },
   "outputs": [],
   "source": [
    "def compute_pixel_distribution(dataloader):\n",
    "    \"\"\"\n",
    "    Compute mean and std for each pixel position in the dataset.\n",
    "    Args:\n",
    "        dataloader: DataLoader containing the dataset.\n",
    "    Returns:\n",
    "        mean: Pixel-wise mean\n",
    "        std: Pixel-wise std\n",
    "    \"\"\"\n",
    "    pixel_sum = 0\n",
    "    pixel_sum_squared = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    for images, _ in dataloader:\n",
    "        pixel_sum += images.sum(dim=0)  # Sum across batch\n",
    "        pixel_sum_squared += (images ** 2).sum(dim=0)\n",
    "        num_samples += images.size(0)  # Number of samples in batch\n",
    "\n",
    "    mean = pixel_sum / num_samples\n",
    "    std = torch.sqrt(pixel_sum_squared / num_samples - mean ** 2)\n",
    "    return mean, std\n",
    "\n",
    "def generate_images(mean, std, num_images=10):\n",
    "    \"\"\"\n",
    "    Generate images by sampling from the learned distribution.\n",
    "    Args:\n",
    "        mean: Pixel-wise mean (height, width) or (channels, height, width).\n",
    "        std: Pixel-wise std deviation (same shape as mean).\n",
    "        num_images: Number of images to generate.\n",
    "    Returns:\n",
    "        Tensor of generated images.\n",
    "    \"\"\"\n",
    "    return torch.normal(mean=mean, std=std).unsqueeze(0).repeat(num_images, 1, 1, 1)\n",
    "\n",
    "# make new dataloaders without DDPM specific transforms\n",
    "def get_mnist_dataloader(batch_size=128):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    trainset = datasets.MNIST(root=\"./temp\", train=True, download=True, transform=transform)\n",
    "    return DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def get_cifar10_dataloader(batch_size=128):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    trainset = datasets.CIFAR10(root=\"./temp\", train=True, download=True, transform=transform)\n",
    "    return DataLoader(trainset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zoHFmPun7qaJ",
    "outputId": "ac9e48e6-393d-4874-edca-9f4250f14d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "MPM: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
      "100%|██████████| 104M/104M [00:00<00:00, 162MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA:  MNIST\n",
      "(2500, 2048)\n",
      "(2500, 2048)\n",
      "Mean1:  (2048,)\n",
      "Mean2:  (2048,)\n",
      "diff:  311.23703\n",
      "Fid:  421.49151427009207\n",
      "REAL:\n",
      "DATA:  MNIST\n",
      "Mean1:  (2048,)\n",
      "Mean2:  (2048,)\n",
      "diff:  0.08329368\n",
      "Fid:  4.9017019593237485\n"
     ]
    }
   ],
   "source": [
    "### Example for MNIST ###\n",
    "set_training_seed(7)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                    \"mps\" if torch.backends.mps.is_available() else\n",
    "                    \"cpu\")\n",
    "print(f\"Using Device: {device}\")\n",
    "\n",
    "# MNIST MPM (\"Upper Bound\") (again only evaluated with 2.5K images instead of 10K)\n",
    "data = \"MNIST\"\n",
    "batch_size = 128\n",
    "print(\"MPM: \")\n",
    "dataloader = get_mnist_dataloader(batch_size=batch_size)\n",
    "mean, std = compute_pixel_distribution(dataloader)\n",
    "generated_images = generate_images(mean, std, num_images=2500)\n",
    "generated_images = generated_images.to(device)\n",
    "fid = full_fid(generated_samples=generated_images, data=data, num_images = 2500)\n",
    "print(\"Fid: \", fid)\n",
    "\n",
    "# MNIST REAL (\"Lower Bound\") (again only evaluated with 2.5K images instead of 10K)\n",
    "print(\"REAL:\")\n",
    "inception_model = get_inception_model()\n",
    "real_activations = get_real_image_activations(inception_classifier=inception_model, data=data, num_images=5000)\n",
    "fid = calculate_fid(real_activations[:2500], real_activations[2500:])\n",
    "print(\"Fid: \", fid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBKK2rIG7qaJ"
   },
   "outputs": [],
   "source": [
    "### DCGAN baseline fro CIFAR10 ###\n",
    "from dcgan_cifar import Discriminator, Generator\n",
    "\n",
    "num_gpu = 1 if torch.cuda.is_available() else 0\n",
    "\n",
    "D = Discriminator(ngpu=1).eval()\n",
    "G = Generator(ngpu=1).eval()\n",
    "\n",
    "D.load_state_dict(torch.load('netD_epoch_199.pth'))\n",
    "G.load_state_dict(torch.load('netG_epoch_199.pth'))\n",
    "if torch.cuda.is_available():\n",
    "    D = D.cuda()\n",
    "    G = G.cuda()\n",
    "\n",
    "\n",
    "def generate_fake_images(generator, num_samples=10000, latent_dim=100, device=device):\n",
    "    generator.eval()\n",
    "    z = torch.randn(num_samples, latent_dim, 1, 1, device=device)  # Random noise\n",
    "    with torch.no_grad():\n",
    "        fake_images = generator(z)  # Shape: [num_samples, 1, 28, 28]\n",
    "    return fake_images\n",
    "\n",
    "\n",
    "\n",
    "gen_img = generate_fake_images(G, num_samples=10000, latent_dim=100, device=device)\n",
    "print(np.shape(gen_img))\n",
    "print(\"MODEL: DC GAN\")\n",
    "fid = full_fid(gen_img, data=\"CIFAR10\" ,num_images=10000)\n",
    "print(\"FID:\", fid)\n",
    "\n",
    "mean_is, std_is = calculate_inception_score(samples=gen_img, dataset='CIFAR10', device=device, batch_size=32, splits=10)\n",
    "print(\"IS: \", mean_is)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJUzjDxs7qaJ"
   },
   "source": [
    "## Visualized samples \n",
    "The below shows the functions we used to visualize the samples drawn, with an example shown where we sample and visualize 28 MNIST images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples_cifar10(samples, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), num_cols=math.ceil(math.sqrt(25))): \n",
    "    \"\"\"\n",
    "    Visualize the generated samples in multiple rows.\n",
    "    Args:\n",
    "        samples: Generated samples of shape (num_samples, 3, 32, 32).\n",
    "        mean: Tuple of mean values used for normalization (default for CIFAR is (0.5, 0.5, 0.5)).\n",
    "        std: Tuple of std values used for normalization (default for CIFAR is (0.5, 0.5, 0.5)).\n",
    "        num_cols: Number of images per row.\n",
    "    \"\"\"\n",
    "    # Denormalize the images\n",
    "    mean = torch.tensor(mean).view(1, 3, 1, 1).to(samples.device)\n",
    "    std = torch.tensor(std).view(1, 3, 1, 1).to(samples.device)\n",
    "    samples = samples * std + mean  # Denormalize\n",
    "\n",
    "    # Clip values to [0, 1] range for display\n",
    "    samples = samples.clamp(0, 1)\n",
    "\n",
    "    num_samples = samples.size(0)\n",
    "    num_rows = math.ceil(num_samples / num_cols)  # Calculate the number of rows\n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))\n",
    "\n",
    "    # Flatten axs array for easier iteration, even if it's a single row\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        sample = samples[i].permute(1, 2, 0).cpu().numpy()  # Convert (3, 32, 32) -> (32, 32, 3)\n",
    "        axs[i].imshow(sample)\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    # Turn off axes for unused subplots\n",
    "    for j in range(num_samples, len(axs)):\n",
    "        axs[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_samples_mnist(samples, mean=(0.5), std=(0.5), num_cols=math.ceil(math.sqrt(25))): \n",
    "    \"\"\"\n",
    "    Visualize the generated samples in multiple rows.\n",
    "    Args:\n",
    "        samples: Generated samples of shape (num_samples, 3, 32, 32).\n",
    "        mean: Tuple of mean values used for normalization (default for CIFAR is (0.5, 0.5, 0.5)).\n",
    "        std: Tuple of std values used for normalization (default for CIFAR is (0.5, 0.5, 0.5)).\n",
    "        num_cols: Number of images per row.\n",
    "    \"\"\"\n",
    "    # Denormalize the images\n",
    "    mean = torch.tensor(mean).view(1, 1, 1, 1).to(samples.device)\n",
    "    std = torch.tensor(std).view(1, 1, 1, 1).to(samples.device)\n",
    "    samples = samples * std + mean  # Denormalize\n",
    "\n",
    "    # Clip values to [0, 1] range for display\n",
    "    samples = samples.clamp(0, 1)\n",
    "\n",
    "    num_samples = samples.size(0)\n",
    "    num_rows = math.ceil(num_samples / num_cols)  # Calculate the number of rows\n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))\n",
    "\n",
    "    # Flatten axs array for easier iteration, even if it's a single row\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        sample = samples[i].permute(1, 2, 0).cpu().numpy()  # Convert (3, 32, 32) -> (32, 32, 3)\n",
    "        axs[i].imshow(sample, cmap='gray')\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    # Turn off axes for unused subplots\n",
    "    for j in range(num_samples, len(axs)):\n",
    "        axs[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sb/klhtpy1n7jd649__g8r7xj300000gn/T/ipykernel_62026/3187388759.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 13756929\n",
      "Trainable parameters: 13756929\n",
      "Sampling 28 samples...\n",
      "Samples generated successfully!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 28 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                      \"mps\" if torch.backends.mps.is_available() else \n",
    "                      \"cpu\")\n",
    "print(f\"Using Device: {device}\")\n",
    "\n",
    "# Step 1: Initialize the Sinusoidal Embeddings\n",
    "time_embedding = SinusoidalPositionEmbeddings(total_time_steps=1000, time_emb_dims=128, time_emb_dims_exp=512).to(device)\n",
    "\n",
    "# Step 2: Initialize the U-Net and Diffusion\n",
    "unet = UNet(input_channels=1, resolutions=[64, 128, 256, 512], time_emb_dims=512, dropout=0.1, use_attention=[False, True, False], heads=4).to(device)\n",
    "diffusion = Diffusion(T=1000, beta_min=10e-5, beta_max=0.02, schedule='cosine', device=device)\n",
    "\n",
    "# Step 3: Load the trained model\n",
    "#model_path = \"ddpm_CIFAR10_linear_heads_1_LRs_none_seed7.pth\"\n",
    "model_path = \"trained_models/ddpm_MNIST_cosine_final.pth\"\n",
    "saved = torch.load(model_path, map_location=device)\n",
    "\n",
    "unet.load_state_dict(saved[\"model_state_dict\"])\n",
    "time_embedding.load_state_dict(saved[\"embedding_state_dict\"])\n",
    "\n",
    "# Number of params (TEST)\n",
    "total_params = sum(p.numel() for p in unet.parameters())\n",
    "trainable_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "# Step 4: Generate samples\n",
    "samples = sample_ddpm(unet, diffusion, time_embedding, device, num_samples=28, dataset='MNIST')\n",
    "print(\"Samples generated successfully!\")\n",
    "#visualize_samples_cifar10(samples, num_cols=8)\n",
    "visualize_samples_mnist(samples, num_cols=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TY9eTwK7qaJ"
   },
   "source": [
    "See plots_for_report notebook for further plots used in the report, such as the FID-during-training plots etc. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ddpm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
